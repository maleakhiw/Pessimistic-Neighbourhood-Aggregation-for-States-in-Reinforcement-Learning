{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pessimistic Neighbourhood Aggregation for States in Reinforcement Learning (Dynamic Grid)\n",
    "\n",
    "*Author: Maleakhi Agung Wijaya  \n",
    "Supervisors: Marcus Hutter, Sultan Javed Majeed  \n",
    "Date Created: 21/12/2017*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set grid color for seaborn\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mountain Car Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mountain Car** is a standard testing domain in Reinforcement Learning, in which an under-powered car must drive up a steep hill. Since gravity is stronger than the car's engine, even at full throttle, the car cannot simply accelerate up the steep slope. The car is situated in a valley and must learn to leverage potential energy by driving up the opposite hill before the car is able to make it to the goal at the top of the rightmost hill.\n",
    "\n",
    "**Technical Details**\n",
    "- *State:* feature vectors consisting of velocity and position represented by an array [velocity, position]\n",
    "- *Reward:* -1 for every step taken, 0 for achieving the goal\n",
    "- *Action:* (left, neutral, right) represented by (-1, 0, 1)\n",
    "- *Initial state:* velocity = 0.0, position = -0.5 represented by [0.0, -0.5]\n",
    "- *Terminal state:* position >= 0.6\n",
    "- *Boundaries:* velocity = (-0.07, 0.07), position = (-1.2, 0.6)\n",
    "- *Update function:* velocity = velocity + (Action) \\* 0.001 + cos(3\\*Position) * (-0.0025), position = position + velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MountainCarEnvironment:\n",
    "    \"\"\"\n",
    "    Description: Environment for Mountain Car problem, adapted from Sutton and Barto's Introduction to Reinforcement Learning.\n",
    "    Author: Maleakhi Agung Wijaya\n",
    "    \"\"\"\n",
    "    VELOCITY_BOUNDARIES = (-0.07, 0.07)\n",
    "    POSITION_BOUNDARIES = (-1.2, 0.6) \n",
    "    \n",
    "    INITIAL_VELOCITY = 0.0\n",
    "    INITIAL_POSITION = -0.5\n",
    "    \n",
    "    REWARD_STEP = -1\n",
    "    REWARD_TERMINAL = 0\n",
    "   \n",
    "    # Constructor for MountainCarEnvironment\n",
    "    # Input: agent for the MountainCarEnvironment\n",
    "    # Output: MountainCarEnvironment object\n",
    "    def __init__(self, car):\n",
    "        self.car = car\n",
    "        self.reset()\n",
    "        \n",
    "    # Compute next state (feature)\n",
    "    # Output: [new velocity, new position]\n",
    "    def nextState(self, action):\n",
    "        # Get current state (velocity, position) and the action chosen by the agent\n",
    "        velocity = self.car.state[0]\n",
    "        position = self.car.state[1]\n",
    "        \n",
    "        # Calculate the new velocity and new position\n",
    "        velocity += action * 0.001 + math.cos(3*position) * (-0.0025)\n",
    "        # Consider boundary for velocity\n",
    "        if (velocity < MountainCarEnvironment.VELOCITY_BOUNDARIES[0]):\n",
    "            velocity = MountainCarEnvironment.VELOCITY_BOUNDARIES[0]\n",
    "        elif (velocity > MountainCarEnvironment.VELOCITY_BOUNDARIES[1]):\n",
    "            velocity = MountainCarEnvironment.VELOCITY_BOUNDARIES[1]\n",
    "            \n",
    "        position += velocity\n",
    "        # Consider boundary for position\n",
    "        if (position < MountainCarEnvironment.POSITION_BOUNDARIES[0]):\n",
    "            position = MountainCarEnvironment.POSITION_BOUNDARIES[0]\n",
    "            velocity = 0\n",
    "        elif (position > MountainCarEnvironment.POSITION_BOUNDARIES[1]):\n",
    "            position = MountainCarEnvironment.POSITION_BOUNDARIES[1]\n",
    "        \n",
    "        new_state = [velocity, position]\n",
    "        return(new_state)\n",
    "    \n",
    "    # Reset to the initial state   \n",
    "    def reset(self):\n",
    "        self.car.state[0] = MountainCarEnvironment.INITIAL_VELOCITY\n",
    "        self.car.state[1] = MountainCarEnvironment.INITIAL_POSITION\n",
    "        \n",
    "    # Give reward for each of the chosen action, depending on what the next state that the agent end up in\n",
    "    # Output: terminal state = 0, non-terminal state = -1\n",
    "    def calculateReward(self):\n",
    "        # Get current position of the agent\n",
    "        position = self.car.state[1]\n",
    "        \n",
    "        # Determine the reward given\n",
    "        if (position >= MountainCarEnvironment.POSITION_BOUNDARIES[1]):\n",
    "            return(MountainCarEnvironment.REWARD_TERMINAL)\n",
    "        else:\n",
    "            return(MountainCarEnvironment.REWARD_STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PNA Agent (Dynamic Grid Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PNA** may be viewed as a refinement for kNN, with k adapting to the situation. On the one hand, it is beneficial to use large k since that means large data can be learn from. On the other hand, it is beneficial to learn only from the most similar past experiences (small k), as the data they provide should be the most relevant. \n",
    "\n",
    "PNA suggests that when predicting the value of an action a in a state s, k should be chosen dynamically to minimise:\n",
    "\n",
    "![equation](pictures/variance.jpg)\n",
    "\n",
    "where c = 1 and Var(Nsa) is the variance of observed rewards in the neighbourhood Nsa. This is a negative version of the term endorsing exploration in the UCB algorithm. Here it promotes choosing neighbourhoods that contain as much data as possible but with small variation between rewards. For example, in the ideal choice of k, all k nearest neighbours of (s, a) behave similarly, but actions farther away behave very differently. \n",
    "\n",
    "Action are chosen optimistically according to the UCB:\n",
    "\n",
    "![equation](pictures/action_selection.jpg)  \n",
    "\n",
    "with c > 0 a small constant. The upper confidence bound is composed of two terms: The first terms is the estimated value, and the second term is an exploration bonus for action whose value is uncertain. Actions can have uncertain value either because they have rarely been selected or have a high variance among previous returns. Meanwhile, the neighbourhoods are chosen \"pessimistically\" for each action to minimise the exploration bonus.\n",
    "\n",
    "**Algorithm:**\n",
    "1. Cover the whole state space by some initial Q(s,a) pairs, possibly scatter it uniformly across the whole state space and give an initial value of 0/ -1\n",
    "2. When an agent in a particular state, get the feature vectors representing the state and possible actions from the state\n",
    "3. For each possible action from the state, calculate Q(s,a) pairs by taking the expected value from previous Q values based on k-nearest neighbours of a particular action. With PNA, we also need to dynamically consider the k values  \n",
    "*Steps for PNA:*\n",
    "    - Standardise every feature in the feature vectors to (-1, 1) or other ranges to make sure that one feature scale not dominate the distance calculation (i.e. if position ranges between (-50, 50) and velocity (-0.7, 0.7) position will dominate distance calculation).\n",
    "    - Calculate the distance between current state and all of other points with the same action using distance formula (i.e. Euclidean distance) and sort based on the closest distance\n",
    "    - Determine k by minimising the variance function described above\n",
    "    - Store the k-nearest neighbours to knn vector, and it's distance (for weight) in weight vector\n",
    "    - Determine the probability p(x) for the expected value by using weight calculation (i.e. weight = 1/distance). To calculate weight, one can use other formula as long as that formula gives more weight to closer point. To calculate p(x) just divide individual weight with sum of all weights to get probability\n",
    "    - Estimate the Q(s,a) pairs using expectation formula from kNN previous Q values\n",
    "4. As we have Q values for every action, we will pop the first 3 elements on the Q storage and append these new calculated Q values into the Q storage\n",
    "5. Using epsilon greedy/ UCB/ other decision methods to choose the next move\n",
    "6. Repeat step 2-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PNAAgent:\n",
    "    \"\"\"\n",
    "    Description: Mountain Car problem agent based on PNA algorithm adapted from Marcus Hutter's literatures\n",
    "    Author: Maleakhi Agung Wijaya\n",
    "    \"\"\"\n",
    "    INITIAL_VELOCITY = 0.0\n",
    "    INITIAL_POSITION = -0.5\n",
    "    INITIAL_VALUE = -1\n",
    "    \n",
    "    ACTIONS = [-1, 0, 1]\n",
    "    GAMMA = 0.995\n",
    "    C = 0.01 # UCB constant\n",
    "    EPSILON = 0.05\n",
    "    \n",
    "    INDEX_DISTANCE = 0\n",
    "    INDEX_ORIGINAL = 1\n",
    "    INDEX_WEIGHT = 2\n",
    "    \n",
    "    REWARD_STEP = -1\n",
    "    REWARD_TERMINAL = 0\n",
    "    \n",
    "    # Constructor\n",
    "    # Input: size of the storage for previous Q values\n",
    "    def __init__(self, size):\n",
    "        self.state = [PNAAgent.INITIAL_VELOCITY, PNAAgent.INITIAL_POSITION]\n",
    "        self.q_storage = []\n",
    "        self.alpha = 1 # will be decaying and change later\n",
    "        \n",
    "        self.k_history = []\n",
    "        \n",
    "        # Initialise the storage with random point \n",
    "        for i in range(size):\n",
    "            initial_action = random.randint(-1, 1)\n",
    "            initial_state = [random.uniform(-0.07, 0.07), random.uniform(-1.2, 0.6)]\n",
    "            \n",
    "            # Each data on the array will consist of state, action pair + value\n",
    "            data = {\"state\": initial_state, \"value\": PNAAgent.INITIAL_VALUE, \"action\": initial_action}\n",
    "            self.q_storage.append(data)\n",
    "    \n",
    "    # Standardise feature vector given\n",
    "    # Input: feature vector to be standardised\n",
    "    # Output: standardised feature vector\n",
    "    def standardiseState(self, state):\n",
    "        standardised_state = []\n",
    "        standardised_velocity = 2 * ((state[0]+0.07) / (0.07+0.07)) - 1\n",
    "        standardised_position = 2 * ((state[1]+1.2) / (0.6+1.2)) - 1\n",
    "        standardised_state.append(standardised_velocity)\n",
    "        standardised_state.append(standardised_position)\n",
    "        \n",
    "        return(standardised_state)\n",
    "    \n",
    "    # Find all index for a given value\n",
    "    # Input: value, list to search\n",
    "    # Output: list of all index where you find that value on the list\n",
    "    def findAllIndex(self, value, list_value):\n",
    "        indices = []\n",
    "        for i in range(len(list_value)):\n",
    "              if (value == list_value[i]):\n",
    "                    indices.append(i)\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    # Calculate Euclidean distance between 2 vectors\n",
    "    # Input: 2 feature vectors\n",
    "    # Output: distance between them\n",
    "    def calculateDistance(self, vector1, vector2):\n",
    "        return(math.sqrt((vector1[0]-vector2[0])**2 + (vector1[1]-vector2[1])**2))\n",
    "    \n",
    "    # Calculate total weight\n",
    "    # Input: list of weights\n",
    "    # Output: total weight\n",
    "    def calculateTotalWeight(self, weight_list):\n",
    "        total_weight = 0\n",
    "        for i in range(len(weight_list)):\n",
    "            total_weight += weight_list[i][PNAAgent.INDEX_WEIGHT]\n",
    "        \n",
    "        return(total_weight)\n",
    "            \n",
    "    # Clear the knn list, k_history, and also the weight list\n",
    "    def cleanList(self):\n",
    "        self.k_history = []\n",
    "    \n",
    "    # Choose the appropriate k by minimising variance and maximising the number of data to learn\n",
    "    # Input: sorted neighbourhood list based on distance (distance, index, weight)\n",
    "    # Output: k (numbers of nearest neighbour) that minimise neighbourhood variance function\n",
    "    def chooseK(self, neighbourhood_list):\n",
    "        data_list = []\n",
    "        # Extract the data (Q value from the neighbourhood_list) and append it to the data_list\n",
    "        for data in neighbourhood_list:\n",
    "            data_list.append(self.q_storage[data[PNAAgent.INDEX_ORIGINAL]][\"value\"])\n",
    "            action = self.q_storage[data[PNAAgent.INDEX_ORIGINAL]][\"action\"]\n",
    "            \n",
    "        # Initialise minimum variance\n",
    "        minimum_k = 2 # Variable that will be return that minimise the variance of the neighbourhood\n",
    "        minimum_function = self.neighbourhoodVariance(1, data_list[:2])\n",
    "        \n",
    "        previous_sum_variance = np.var(data_list[:2]) * 2\n",
    "        previous_mean = np.mean(data_list[:2])\n",
    "        k = 2\n",
    "        # Iterate to find optimal k that will minimise the neighbourhood variance function\n",
    "        for i in range(2, len(neighbourhood_list)):\n",
    "            target_x = data_list[i]\n",
    "            mean = (previous_mean * k + target_x) / (k + 1)\n",
    "            current_sum_variance = previous_sum_variance + (target_x - previous_mean) * (target_x - mean)\n",
    "            \n",
    "            # Update for next iteration\n",
    "            k = k + 1\n",
    "            previous_sum_variance = current_sum_variance\n",
    "            previous_mean = mean\n",
    "            \n",
    "            function = self.neighbourhoodVariance(1, [], previous_sum_variance / k, k)\n",
    "            # Update the k value and minimum var value if find parameter which better minimise than the previous value\n",
    "            if (function <= minimum_function):\n",
    "                minimum_k = k\n",
    "                minimum_function = function\n",
    "            \n",
    "        return(minimum_k)\n",
    "    \n",
    "    # PNA variance function that needed to be minimise\n",
    "    # Input: constant c, list containing data points\n",
    "    # Output: calculation result from the neighbourhood variance function\n",
    "    def neighbourhoodVariance(self, c, data_list, var = None, k = None):\n",
    "        if (var == None):\n",
    "            return(math.sqrt(c * np.var(data_list) / len(data_list)))\n",
    "        else:\n",
    "            return(math.sqrt(c * var / k))\n",
    "    \n",
    "    # Store Q values to Q storage and pop items when storing\n",
    "    # Input: Q values for each action, state\n",
    "    def processStorage(self, action_values, state):\n",
    "        # Make the point\n",
    "        for i in range(len(action_values)):\n",
    "            data = {}\n",
    "            data[\"action\"] = i - 1\n",
    "            data[\"value\"] = action_values[i]\n",
    "            data[\"state\"] = state\n",
    "            \n",
    "            # Pop and append the data into q_storage\n",
    "            self.q_storage.pop(0)\n",
    "            self.q_storage.append(data)\n",
    "                \n",
    "    # Get starting index for the weight list\n",
    "    # Input: action, k_history\n",
    "    # Output: starting index for the weight list\n",
    "    def getStartingIndex(self, action, k_history):\n",
    "        count_action = action + 1\n",
    "        if (count_action == 0):\n",
    "            return(0)\n",
    "        else:\n",
    "            index = 0\n",
    "            for i in range(count_action):\n",
    "                index += k_history[i]\n",
    "            return(index)\n",
    "    \n",
    "    # Apply the PNA algorithm for feature vector and store the data point on the neighbours array\n",
    "    # Input: feature vector of current state, actions array consisting of all possible actions, list that will store knn data and weights data, k_history\n",
    "    # Output: vector containing the value of taking each action (left, neutral, right)\n",
    "    def PNA(self, state, actions, knn_list, weight_list, k_history):\n",
    "        approximate_action = []\n",
    "        \n",
    "        # Get the standardised version of state\n",
    "        standardised_state = self.standardiseState(state)\n",
    "        \n",
    "        # Loop through every element in the storage array and only calculate for particular action\n",
    "        for action in actions:\n",
    "            temp = [] # array consisting of tuple (distance, original index, weight) for each point in the q_storage\n",
    "            for i in range(len(self.q_storage)):\n",
    "                data = self.q_storage[i]\n",
    "                # Only want to calculate the nearest neighbour state which has the same action\n",
    "                if (data[\"action\"] == action):\n",
    "                    vector_2 = data[\"state\"]\n",
    "                    standardised_vector_2 = self.standardiseState(vector_2)\n",
    "                    distance = self.calculateDistance(standardised_state, standardised_vector_2)\n",
    "                    index = i\n",
    "                    weight = 1 / (1+distance**2)\n",
    "            \n",
    "                    # Create the tuple and append that to temp\n",
    "                    temp.append(tuple((distance, index, weight)))\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            # After we finish looping through all of the point and calculating the standardise distance,\n",
    "            # Sort the tuple based on the distance and only take k of it and append that to the neighbours array\n",
    "            sorted_temp = sorted(temp, key=lambda x: x[0])\n",
    "            # Get the value of the k dynamically\n",
    "            k = self.chooseK(sorted_temp)\n",
    "            k_history.append(k)\n",
    "            \n",
    "            for i in range(k):\n",
    "                try:\n",
    "                    weight_list.append(sorted_temp[i])\n",
    "                    knn_list.append(self.q_storage[sorted_temp[i][PNAAgent.INDEX_ORIGINAL]])\n",
    "                except IndexError:\n",
    "                    sys.exit(0)\n",
    "            \n",
    "            # Calculate the expected value of the action and append it to the approximate_action array\n",
    "            expected_value = 0\n",
    "            # We also need to calculate the total weight to make it into valid probability that we can compute it's expectation\n",
    "            total_weight = self.calculateTotalWeight(weight_list[self.getStartingIndex(action, k_history):self.getStartingIndex(action, k_history)+k])\n",
    "            for i in range(self.getStartingIndex(action, k_history), self.getStartingIndex(action, k_history) + k):\n",
    "                try:\n",
    "                    weight = weight_list[i][PNAAgent.INDEX_WEIGHT]\n",
    "                    probability = weight / total_weight\n",
    "                    expected_value += probability * knn_list[i][\"value\"]\n",
    "                except IndexError:\n",
    "                    sys.exit(0)\n",
    "                    \n",
    "            approximate_action.append(expected_value)\n",
    "        \n",
    "        return(approximate_action)\n",
    "    \n",
    "    # Choosing based on Epsilon Greedy method\n",
    "    # Input: action_value array consisting the Q value of every action\n",
    "    # Output: action chosen (-1, 0, 1)\n",
    "    def epsilonGreedy(self, action_value):\n",
    "        # Use the epsilon-greedy method to choose value\n",
    "        random_number = random.uniform(0.0, 1.0)\n",
    "        if (random_number <= PNAAgent.EPSILON):\n",
    "            action_chosen = random.randint(-1, 1)\n",
    "        else:\n",
    "            # Return the action with highest Q(s,a)\n",
    "            possible_index = self.findAllIndex(max(action_value), action_value)\n",
    "            action_chosen = possible_index[random.randrange(len(possible_index))] - 1\n",
    "        \n",
    "        return action_chosen\n",
    "    \n",
    "    # Getting the maximum of the ucb method\n",
    "    # Input: action_value list, bonus_variance list\n",
    "    # Output: action which maximise\n",
    "    def maximumUCB(self, action_value, bonus_variance):\n",
    "        max_index = 0\n",
    "        max_value = action_value[0] + bonus_variance[0]\n",
    "        \n",
    "        # Check 1, 2 (all possible action)\n",
    "        for i in range(1, 3):\n",
    "            value = action_value[i] + bonus_variance[i]\n",
    "            \n",
    "            if (value >= max_value):\n",
    "                max_value = value\n",
    "                max_index = i\n",
    "        \n",
    "        return(max_index - 1) # return the action which maximise\n",
    "        \n",
    "    # Select which action to choose, whether left, neutral, or right (using UCB)\n",
    "    # Output: -1 (left), 0 (neutral), 1 (right)\n",
    "    def selectAction(self):\n",
    "        knn = []\n",
    "        weight = []\n",
    "        action_value = self.PNA(self.state, PNAAgent.ACTIONS, knn, weight, self.k_history)\n",
    "        \n",
    "        # Modify the q_storage by popping old item and inserting new item\n",
    "        self.processStorage(action_value, self.state)\n",
    "            \n",
    "        # Choose the action based on ucb method\n",
    "        action_chosen = self.epsilonGreedy(action_value)\n",
    "        \n",
    "        # Clean\n",
    "        self.cleanList()\n",
    "        \n",
    "        return action_chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PNA Main Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PNA Main function** is responsible for initiating the PNA agent, environment and handling agent-environment interaction. It consists of a non-terminate inner loop that direct agent decision while also giving reward and next state from the environment. This inner loop will only break after the agent successfully get out of the environment, which in this case the mountain or if it is taking too long to converge. The outer loop can also be created to control the number of episodes which the agent will perform before the main function ends.\n",
    "\n",
    "Apart from handling agent-environment interaction, main function also responsible to display five kinds of visualisation. First, table/ DataFrame displaying episodes and step that are required by the agent to get out of the mountain on each episode. Second, scatter plot displaying steps on the y axis and episodes on the x axis to learn about algorithm convergence property. Third, expected standard error function for every actions. Fourth, heatmap of the Q value for the last episode. Lastly, as the k is dynamically changing each steps, I have created a heatmap indicating k chosen each steps for first episode and last episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate decaying alphas\n",
    "# Input: minimum alpha, number of episodes\n",
    "# Output: list containing alpha\n",
    "def generateAlphas(minimum_alpha, n_episodes):\n",
    "    return(np.linspace(1.0, MIN_ALPHA, N_EPISODES))\n",
    "\n",
    "N_EPISODES = 1000\n",
    "MIN_ALPHA = 0.02\n",
    "alphas = generateAlphas(MIN_ALPHA, N_EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialise the environment and the agent\n",
    "size = 1000 # size of the q_storage \n",
    "agent = PNAAgent(size)\n",
    "mountain_car_environment = MountainCarEnvironment(agent)\n",
    "\n",
    "# Used for graphing purposes\n",
    "count_step = [] # counter for how many step in each episodes\n",
    "\n",
    "k_first_left = []\n",
    "k_first_neutral = []\n",
    "k_first_right = []\n",
    "\n",
    "k_last_left = []\n",
    "k_last_neutral = []\n",
    "k_last_right = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate the process, train the agent (training_iteration episodes)\n",
    "total_step = 0\n",
    "training_iteration = N_EPISODES\n",
    "for i in range(training_iteration):\n",
    "    step = 0\n",
    "    alpha = alphas[i]\n",
    "    mountain_car_environment.reset()\n",
    "    while (True):\n",
    "        action = agent.selectAction()\n",
    "        next_state = mountain_car_environment.nextState(action)\n",
    "        \n",
    "        # Change agent current state and getting reward\n",
    "        agent.state = next_state\n",
    "        immediate_reward = mountain_car_environment.calculateReward()\n",
    "        \n",
    "        # Used for graphing\n",
    "        step += 1\n",
    "        total_step += 1\n",
    "        \n",
    "        # Only append first and last episode (for the k)\n",
    "        if (i == 1):\n",
    "            k_first_left.append(agent.k_history[0])\n",
    "            k_first_neutral.append(agent.k_history[1])\n",
    "            k_first_right.append(agent.k_history[2])\n",
    "        if (i == (training_iteration - 1)):\n",
    "            k_last_left.append(agent.k_history[0])\n",
    "            k_last_neutral.append(agent.k_history[1])\n",
    "            k_last_right.append(agent.k_history[2])\n",
    "        \n",
    "        # Test for successful learning\n",
    "        if (immediate_reward == MountainCarEnvironment.REWARD_TERMINAL):\n",
    "            count_step.append(step)\n",
    "            \n",
    "            clear_output(wait=True) # clear previous output\n",
    "            # Create table\n",
    "            d = {\"Steps\": count_step}\n",
    "            episode_table = pd.DataFrame(data=d, index=np.arange(1, len(count_step)+1))\n",
    "            episode_table.index.names = ['Episodes']\n",
    "            display(episode_table)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create graph for step vs episodes\n",
    "y = count_step\n",
    "x = np.arange(1, len(y) + 1)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Steps vs Episodes (Log Scale)\", fontsize=16)\n",
    "plt.xlabel(\"Episodes (Log)\")\n",
    "plt.ylabel(\"Steps (Log)\")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create heatmap for Q values\n",
    "data = pd.DataFrame()\n",
    "data_left = []\n",
    "data_neutral = []\n",
    "data_right = []\n",
    "\n",
    "position_left = []\n",
    "position_neutral = []\n",
    "position_right = []\n",
    "\n",
    "velocity_left = []\n",
    "velocity_neutral = []\n",
    "velocity_right = []\n",
    "\n",
    "# Sort q_storage based on position and velocity\n",
    "q_storage_sorted = sorted(agent.q_storage, key=lambda k: k['state'][0])  \n",
    "# Separate action left, neutral, and right\n",
    "for elem in q_storage_sorted:\n",
    "    if (elem[\"action\"] == -1):\n",
    "        data_left.append(elem[\"value\"])\n",
    "        position_left.append(elem[\"state\"][1])\n",
    "        velocity_left.append(elem[\"state\"][0])\n",
    "    elif (elem[\"action\"] == 0):\n",
    "        data_neutral.append(elem[\"value\"])\n",
    "        position_neutral.append(elem[\"state\"][1])\n",
    "        velocity_neutral.append(elem[\"state\"][0])\n",
    "    else:\n",
    "        data_right.append(elem[\"value\"])\n",
    "        position_right.append(elem[\"state\"][1])\n",
    "        velocity_right.append(elem[\"state\"][0])\n",
    "\n",
    "# Make scatter plot for 3 actions (left, neutral, right)\n",
    "# Left\n",
    "plt.scatter(x=velocity_left, y=position_left, c=data_left, cmap=\"YlGnBu\")\n",
    "plt.title(\"Q Values (Action Left)\", fontsize=16)\n",
    "plt.xlabel(\"Velocity\")\n",
    "plt.ylabel(\"Position\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Neutral\n",
    "plt.scatter(x=velocity_neutral, y=position_neutral, c=data_neutral, cmap=\"YlGnBu\")\n",
    "plt.title(\"Q Values (Action Neutral)\", fontsize=16)\n",
    "plt.xlabel(\"Velocity\")\n",
    "plt.ylabel(\"Position\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Right\n",
    "plt.scatter(x=velocity_right, y=position_right, c=data_right, cmap=\"YlGnBu\")\n",
    "plt.title(\"Q Values (Action Right)\", fontsize=16)\n",
    "plt.xlabel(\"Velocity\")\n",
    "plt.ylabel(\"Position\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create heatmap showing the k (first episode)\n",
    "data = pd.DataFrame()\n",
    "data[\"Action Left\"] = k_first_left\n",
    "data[\"Action Neutral\"] = k_first_neutral\n",
    "data[\"Action Right\"] = k_first_right\n",
    "data[\"Steps\"] = np.arange(1, len(k_first_left) + 1)\n",
    "data.set_index(\"Steps\", inplace=True)\n",
    "\n",
    "grid_kws = {\"height_ratios\": (.9, .05), \"hspace\": .3}\n",
    "f, (ax, cbar_ax) = plt.subplots(2, gridspec_kw=grid_kws)\n",
    "ax = sns.heatmap(data, ax=ax, cbar_ax=cbar_ax, cbar_kws={\"orientation\": \"horizontal\"}, yticklabels=False)\n",
    "ax.set_title(\"Number of K Chosen Each Step (First Episode)\", fontsize=16)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Create heatmap showing the k (last episode)\n",
    "data = pd.DataFrame()\n",
    "data[\"Action Left\"] = k_last_left\n",
    "data[\"Action Neutral\"] = k_last_neutral\n",
    "data[\"Action Right\"] = k_last_right\n",
    "data[\"Steps\"] = np.arange(1, len(k_last_left) + 1)\n",
    "data.set_index(\"Steps\", inplace=True)\n",
    "\n",
    "grid_kws = {\"height_ratios\": (.9, .05), \"hspace\": .3}\n",
    "f, (ax, cbar_ax) = plt.subplots(2, gridspec_kw=grid_kws)\n",
    "ax = sns.heatmap(data, ax=ax, cbar_ax=cbar_ax, cbar_kws={\"orientation\": \"horizontal\"}, yticklabels=False)\n",
    "ax.set_title(\"Number of K Chosen Each Step (Last Episode)\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
