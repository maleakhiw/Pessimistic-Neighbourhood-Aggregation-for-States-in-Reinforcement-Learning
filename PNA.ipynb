{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pessimistic Neighbourhood Aggregation for States in Reinforcement Learning\n",
    "\n",
    "*Author: Maleakhi Agung Wijaya  \n",
    "Supervisors: Marcus Hutter, Sultan Javed Majeed  \n",
    "Date Created: 21/12/2017*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mountain Car Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mountain Car** is a standard testing domain in Reinforcement Learning, in which an under-powered car must drive up a steep hill. Since gravity is stronger than the car's engine, even at full throttle, the car cannot simply accelerate up the steep slope. The car is situated in a valley and must learn to leverage potential energy by driving up the opposite hill before the car is able to make it to the goal at the top of the rightmost hill.\n",
    "\n",
    "**Technical Details**\n",
    "- *State:* feature vectors consisting of velocity and position represented by an array [velocity, position]\n",
    "- *Reward:* -1 for every step taken, 0 for achieving the goal\n",
    "- *Action:* (left, neutral, right) represented by (-1, 0, 1)\n",
    "- *Initial state:* velocity = 0.0, position = -0.5 represented by [0.0, -0.5]\n",
    "- *Terminal state:* position >= 0.6\n",
    "- *Boundaries:* velocity = (-0.07, 0.07), position = (-1.2, 0.6)\n",
    "- *Update function:* velocity = velocity + (Action) \\* 0.001 + cos(3\\*Position) * (-0.0025), position = position + velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MountainCarEnvironment:\n",
    "    VELOCITY_BOUNDARIES = (-0.07, 0.07)\n",
    "    POSITION_BOUNDARIES = (-1.2, 0.6) \n",
    "    \n",
    "    INITIAL_VELOCITY = 0.0\n",
    "    INITIAL_POSITION = -0.5\n",
    "    \n",
    "    REWARD_STEP = -1\n",
    "    REWARD_TERMINAL = 0\n",
    "   \n",
    "    # Constructor for MountainCarEnvironment\n",
    "    # Input: agent for the MountainCarEnvironment\n",
    "    # Output: MountainCarEnvironment object\n",
    "    def __init__(self, car):\n",
    "        self.car = car\n",
    "        self.reset()\n",
    "        \n",
    "    # Compute next state (feature)\n",
    "    # Output: [new velocity, new position]\n",
    "    def nextState(self, action):\n",
    "        # Get current state (velocity, position) and the action chosen by the agent\n",
    "        velocity = self.car.state[0]\n",
    "        position = self.car.state[1]\n",
    "        \n",
    "        # Calculate the new velocity and new position\n",
    "        velocity += action * 0.001 + math.cos(3*position) * (-0.0025)\n",
    "        # Consider boundary for velocity\n",
    "        if (velocity < MountainCarEnvironment.VELOCITY_BOUNDARIES[0]):\n",
    "            velocity = MountainCarEnvironment.VELOCITY_BOUNDARIES[0]\n",
    "        elif (velocity > MountainCarEnvironment.VELOCITY_BOUNDARIES[1]):\n",
    "            velocity = MountainCarEnvironment.VELOCITY_BOUNDARIES[1]\n",
    "            \n",
    "        position += velocity\n",
    "        # Consider boundary for position\n",
    "        if (position < MountainCarEnvironment.POSITION_BOUNDARIES[0]):\n",
    "            position = MountainCarEnvironment.POSITION_BOUNDARIES[0]\n",
    "        elif (position > MountainCarEnvironment.POSITION_BOUNDARIES[1]):\n",
    "            position = MountainCarEnvironment.POSITION_BOUNDARIES[1]\n",
    "        \n",
    "        new_state = [velocity, position]\n",
    "        return(new_state)\n",
    "    \n",
    "    # Reset to the initial state   \n",
    "    def reset(self):\n",
    "        self.car.state[0] = MountainCarEnvironment.INITIAL_VELOCITY\n",
    "        self.car.state[1] = MountainCarEnvironment.INITIAL_POSITION\n",
    "        \n",
    "    # Give reward for each of the chosen action, depending on what the next state that the agent end up in\n",
    "    # Output: terminal state = 0, non-terminal state = -1\n",
    "    def calculateReward(self):\n",
    "        # Get current position of the agent\n",
    "        position = self.car.state[1]\n",
    "        \n",
    "        # Determine the reward given\n",
    "        if (position >= MountainCarEnvironment.POSITION_BOUNDARIES[1]):\n",
    "            return(MountainCarEnvironment.REWARD_TERMINAL)\n",
    "        else:\n",
    "            return(MountainCarEnvironment.REWARD_STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## KNN-TD Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**kNN-TD** combines the concept of *K-Nearest Neighbours* and *TD-Learning* to learn and evaluate Q values in both continuous and discrete state space RL problems. This method is especially useful in continuous states RL problems as the number of (state, action) pairs is very large and thus impossible to store and learn this information. By choosing a particular k-values and decided some initial points over continuous states, one can estimate Q values based on calculated the weighted average of Q values of the k-nearest neighbours for the state that the agent are currently in and use that values to decide the next move using some decision methods (i.e. UCB or epsilon-greedy). As for the learning process, one can update all of the k-nearest neighbours that contribute for the Q calculation.\n",
    "\n",
    "**Algorithm:**\n",
    "1. Cover the whole state space by some initial Q(s,a) pairs, possibly scatter it uniformly across the whole state space and give an initial value of 0  \n",
    "2. When an agent in a particular state, get the feature vectors representing the state and possible actions from the state\n",
    "3. For each possible action from the state, calculate Q(s,a) pairs by taking the expected value from previous Q values based on k-nearest neighbours of a particular action.  \n",
    "*Steps for k-nearest neighbours:*\n",
    "    - Standardise every feature in the feature vectors to (-1, 1) or other ranges to make sure that 1 feature scaling not dominate the distance calculation\n",
    "    - Calculate the distance between current state and all of other points using distance formula (i.e. Euclidean distance) and store the k-nearest neighbours to knn vector, and it's distance\n",
    "    - Determine the weight (p(x)) for the expected value by using the inverse of the distance\n",
    "    - Estimate the Q(s,a) pairs using expectation formula using the weight and previous Q values of the kNN (average method)\n",
    "4. Using epsilon greedy decision method choose the next move\n",
    "5. Observe the reward and update the Q values for all of the neighbours using SARSA or Q Learning. (on the code below, I use Q Learning)\n",
    "6. Repeat step 2-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class KNNAgent:\n",
    "    INITIAL_VELOCITY = 0.0\n",
    "    INITIAL_POSITION = -0.5\n",
    "    INITIAL_VALUE = -1\n",
    "    \n",
    "    ACTIONS = [-1, 0, 1]\n",
    "    GAMMA = 1\n",
    "    ALPHA = 0.5\n",
    "    EPSILON = 0.05\n",
    "    \n",
    "    INDEX_DISTANCE = 0\n",
    "    INDEX_ORIGINAL = 1\n",
    "    INDEX_WEIGHT = 2\n",
    "    \n",
    "    REWARD_STEP = -1\n",
    "    REWARD_TERMINAL = 0\n",
    "    \n",
    "    # Constructor\n",
    "    # Input: size of the storage for previous Q values, parameters for how many neighbours which the agent will choose\n",
    "    def __init__(self, size, k):\n",
    "        self.state = [KNNAgent.INITIAL_VELOCITY, KNNAgent.INITIAL_POSITION]\n",
    "        self.q_storage = []\n",
    "        self.k = k # fixed number of nearest neighbours that we will used\n",
    "        \n",
    "        # Storage of the k nearest neighbour (data) and weight (inverse of distance) for a particular step\n",
    "        self.knn = []\n",
    "        self.weight = []\n",
    "        \n",
    "        # Initialise the storage with random point \n",
    "        for i in range(size):\n",
    "            initial_action = random.randint(-1, 1)\n",
    "            initial_state = [random.uniform(-0.07, 0.07), random.uniform(-1.2, 0.6)]\n",
    "            \n",
    "            # Each data on the array will consist of state, action pair + value\n",
    "            data = {\"state\": initial_state, \"value\": KNNAgent.INITIAL_VALUE, \"action\": initial_action}\n",
    "            self.q_storage.append(data)\n",
    "    \n",
    "    # Find all index for a given value\n",
    "    # Input: value, list to search\n",
    "    # Output: list of all index where you find that value on the list\n",
    "    def findAllIndex(self, value, list_value):\n",
    "        indices = []\n",
    "        for i in range(len(list_value)):\n",
    "              if (value == list_value[i]):\n",
    "                    indices.append(i)\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    # Standardise feature vector given\n",
    "    # Input: feature vector to be standardised\n",
    "    # Output: standardised feature vector\n",
    "    def standardiseState(self, state):\n",
    "        standardised_state = []\n",
    "        \n",
    "        # The number is taken from VELOCITY_BOUNDARIES and POSITION_BOUNDARIES using normal standardisation formula\n",
    "        standardised_velocity = 2 * ((state[0]+0.07) / (0.07+0.07)) - 1\n",
    "        standardised_position = 2 * ((state[1]+1.2) / (0.6+1.2)) - 1\n",
    "        \n",
    "        standardised_state.append(standardised_velocity)\n",
    "        standardised_state.append(standardised_position)\n",
    "        \n",
    "        return(standardised_state)\n",
    "    \n",
    "    # Calculate Euclidean distance between 2 vectors\n",
    "    # Input: 2 feature vectors\n",
    "    # Output: distance between them\n",
    "    def calculateDistance(self, vector1, vector2):\n",
    "        return(math.sqrt((vector1[0]-vector2[0])**2 + (vector1[1]-vector2[1])**2))\n",
    "    \n",
    "    # Calculate total weight\n",
    "    # Input: list of weights\n",
    "    # Output: total weight\n",
    "    def calculateTotalWeight(self, weight_list):\n",
    "        total_weight = 0\n",
    "        for i in range(len(weight_list)):\n",
    "            total_weight += weight_list[i][KNNAgent.INDEX_WEIGHT]\n",
    "        \n",
    "        return(total_weight)\n",
    "    \n",
    "    # Apply the kNN algorithm for feature vector and store the data point on the neighbours array\n",
    "    # Input: feature vector of current state, actions array consisting of all possible actions, list that will store knn data and weights data\n",
    "    # Output: vector containing the value of taking each action (left, neutral, right)\n",
    "    def kNNTD(self, state, actions, knn_list, weight_list):\n",
    "        approximate_action = []\n",
    "        \n",
    "        # Get the standardised version of state\n",
    "        standardised_state = self.standardiseState(state)\n",
    "        \n",
    "        # Loop through every element in the storage array and only calculate for particular action\n",
    "        for action in actions:\n",
    "            temp = [] # array consisting of tuple (distance, original index, weight) for each point in the q_storage\n",
    "            for i in range(len(self.q_storage)):\n",
    "                data = self.q_storage[i]\n",
    "                # Only want to calculate the nearest neighbour state which has the same action\n",
    "                if (data[\"action\"] == action):\n",
    "                    vector_2 = data[\"state\"]\n",
    "                    standardised_vector_2 = self.standardiseState(vector_2)\n",
    "                    distance = self.calculateDistance(standardised_state, standardised_vector_2)\n",
    "                    index = i\n",
    "                    weight = 1 / (1+distance**2)\n",
    "            \n",
    "                    # Create the tuple and append that to temp\n",
    "                    temp.append(tuple((distance, index, weight)))\n",
    "                else:\n",
    "                    continue\n",
    "        \n",
    "            # After we finish looping through all of the point and calculating the standardise distance,\n",
    "            # Sort the tuple based on the distance and only take k of it and append that to the neighbours array\n",
    "            # We also need to calculate the total weight to make it into valid probability that we can compute it's expectation\n",
    "            sorted_temp = sorted(temp, key=lambda x: x[0])\n",
    "            for i in range(self.k):\n",
    "                try:\n",
    "                    weight_list.append(sorted_temp[i])\n",
    "                    knn_list.append(self.q_storage[sorted_temp[i][KNNAgent.INDEX_ORIGINAL]])\n",
    "                except IndexError:\n",
    "                    sys.exit(0)\n",
    "            \n",
    "            # Calculate the expected value of the action and append it to the approximate_action array\n",
    "            expected_value = 0\n",
    "            total_weight = self.calculateTotalWeight(weight_list[(action+1)*self.k:(action+1)*self.k + self.k])\n",
    "            for i in range((action+1)*self.k, (action+1)*self.k + self.k):\n",
    "                weight = weight_list[i][KNNAgent.INDEX_WEIGHT]\n",
    "                probability = weight / total_weight\n",
    "                expected_value += probability * knn_list[i][\"value\"]\n",
    "                \n",
    "            approximate_action.append(expected_value)\n",
    "        \n",
    "        return(approximate_action)\n",
    "    \n",
    "    # Select which action to choose, whether left, neutral, or right (using epsilon greedy)\n",
    "    # Output: -1 (left), 0 (neutral), 1 (right)\n",
    "    def selectAction(self):\n",
    "        # First call the knn-td algorithm to determine the value of each Q(s,a) pairs\n",
    "        action_value = self.kNNTD(self.state, KNNAgent.ACTIONS, self.knn, self.weight)\n",
    "        \n",
    "        # Use the epsilon-greedy method to choose value\n",
    "        random_number = random.uniform(0.0, 1.0)\n",
    "        if (random_number <= KNNAgent.EPSILON):\n",
    "            action_chosen = random.randint(-1, 1)\n",
    "        else:\n",
    "            # Return the action with highest Q(s,a)\n",
    "            possible_index = self.findAllIndex(max(action_value), action_value)\n",
    "            action_chosen = possible_index[random.randrange(len(possible_index))] - 1\n",
    "        \n",
    "        # Only store chosen data in the knn and weight list\n",
    "        # Clearance step\n",
    "        chosen_knn = []\n",
    "        chosen_weight = []\n",
    "        for i in range(self.k*(action_chosen+1), self.k*(action_chosen+1) + self.k):\n",
    "            chosen_knn.append(self.knn[i])\n",
    "            chosen_weight.append(self.weight[i])\n",
    "        self.knn = chosen_knn\n",
    "        self.weight = chosen_weight\n",
    "\n",
    "        return action_chosen\n",
    "    \n",
    "    # Calculate TD target based on Q Learning/ SARSAMAX\n",
    "    # Input: Immediate reward based on what the environment gave\n",
    "    # Output: TD target based on off policy Q learning\n",
    "    def calculateTDTarget(self, immediate_reward):\n",
    "        # Consider condition on the final state, return 0 immediately\n",
    "        if (immediate_reward == KNNAgent.REWARD_TERMINAL):\n",
    "            return(immediate_reward)\n",
    "        \n",
    "        knn_prime = []\n",
    "        weight_prime = []\n",
    "        action_value = self.kNNTD(self.state, KNNAgent.ACTIONS, knn_prime, weight_prime)\n",
    "        \n",
    "        return(immediate_reward + KNNAgent.GAMMA*max(action_value))\n",
    "    \n",
    "    # Q learning TD updates on every neighbours on the kNN based on the contribution that are calculated using probability weight\n",
    "    # Input: Immediate reward based on what the environment gave\n",
    "    def TDUpdate(self, immediate_reward):\n",
    "        # First, calculate the TD target\n",
    "        td_target = self.calculateTDTarget(immediate_reward)\n",
    "        \n",
    "        # Iterate every kNN and update using Q learning method based on the weighting\n",
    "        total_weight = self.calculateTotalWeight(self.weight)\n",
    "        for i in range(len(self.weight)):\n",
    "            index = self.weight[i][KNNAgent.INDEX_ORIGINAL]\n",
    "            probability = self.weight[i][KNNAgent.INDEX_WEIGHT] / total_weight\n",
    "            \n",
    "            # Begin updating\n",
    "            td_error = td_target - self.q_storage[index][\"value\"]\n",
    "            self.q_storage[index][\"value\"] = self.q_storage[index][\"value\"] + KNNAgent.ALPHA*td_error*probability\n",
    "        \n",
    "        self.cleanList() # clean list to prepare for another step\n",
    "            \n",
    "    # Clear the knn list and also the weight list\n",
    "    def cleanList(self):\n",
    "        self.knn = []\n",
    "        self.weight = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Main Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN Main function** is responsible for initiating the KNN agent, environment and handling agent-environment interaction. It consists of a non-terminate inner loop that direct agent decision while also giving reward and next state from the environment. This inner loop will only break after the agent successfully get out of the environment, which in this case the mountain or if it is taking too long to converge. The outer loop can also be created to control the number of episodes which the agent will perform before the main function ends.\n",
    "\n",
    "Apart from handling agent-environment interaction, main function also responsible to display two kinds of visualisation. First, table/ DataFrame displaying episodes and step that are required by the agent to get out of the mountain on each episode. Second, scatter plot displaying average rewards on the y axis and steps on the x axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Steps</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Episodes</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17860.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34368.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21552.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>22000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>367.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>23041.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>294.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>287.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>257.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>229.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>234.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>426.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>321.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>319.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>335.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>389.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>532.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>37203.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>852.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>624.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>348.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>68577.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>455.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>361.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>368.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>429.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>563.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1273.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>5058.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39368.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>12929.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>15723.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>64588.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>30540.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>41642.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>365.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>307.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>253.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>313.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>316.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3236.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>389.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>753.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Steps\n",
       "Episodes         \n",
       "1         92016.0\n",
       "2         22711.0\n",
       "3         17860.0\n",
       "4         34368.0\n",
       "5         21552.0\n",
       "6         22000.0\n",
       "7         50112.0\n",
       "8           367.0\n",
       "9         23041.0\n",
       "10          294.0\n",
       "11          287.0\n",
       "12          257.0\n",
       "13          229.0\n",
       "14          242.0\n",
       "15          234.0\n",
       "16          426.0\n",
       "17          321.0\n",
       "18          319.0\n",
       "19          335.0\n",
       "20          389.0\n",
       "21          343.0\n",
       "22          532.0\n",
       "23            NaN\n",
       "24        37203.0\n",
       "25          852.0\n",
       "26          624.0\n",
       "27          348.0\n",
       "28        68577.0\n",
       "29          455.0\n",
       "30          450.0\n",
       "31          361.0\n",
       "32          368.0\n",
       "33          429.0\n",
       "34          563.0\n",
       "35         1273.0\n",
       "36         5058.0\n",
       "37        39368.0\n",
       "38        12929.0\n",
       "39        15723.0\n",
       "40        64588.0\n",
       "41        30540.0\n",
       "42        41642.0\n",
       "43          365.0\n",
       "44          307.0\n",
       "45          253.0\n",
       "46          313.0\n",
       "47          316.0\n",
       "48         3236.0\n",
       "49          389.0\n",
       "50          753.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEYCAYAAABlfjCwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1eba9e218d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialise the environment and the agent\n",
    "size = 1000 # size of the q_storage \n",
    "k = 50 # knn parameter\n",
    "agent = KNNAgent(size, k)\n",
    "mountain_car_environment = MountainCarEnvironment(agent)\n",
    "\n",
    "# Used for graphing purposes\n",
    "total_step = 0 # used as x axis for plotting\n",
    "cumulative_average = [] # used as y axis\n",
    "total_sum = 0 # counter used to calculate cumulative average\n",
    "\n",
    "count_step = [] # counter for how many step in each episodes\n",
    "\n",
    "# Iterate the process, train the agent (training_iteration episodes)\n",
    "training_iteration = 50\n",
    "for i in range(training_iteration):\n",
    "    step = 0\n",
    "    mountain_car_environment.reset()\n",
    "    while (True):\n",
    "        action = agent.selectAction()\n",
    "        next_state = mountain_car_environment.nextState(action)\n",
    "        \n",
    "        # Change agent current state and getting reward\n",
    "        agent.state = next_state\n",
    "        immediate_reward = mountain_car_environment.calculateReward()\n",
    "        \n",
    "        # Used for graphing\n",
    "        step += 1\n",
    "        total_step += 1\n",
    "        total_sum += immediate_reward        \n",
    "        cumulative_average.append(total_sum / total_step)\n",
    "        \n",
    "        # Test for successful learning\n",
    "        if (immediate_reward == MountainCarEnvironment.REWARD_TERMINAL):\n",
    "            agent.TDUpdate(immediate_reward)\n",
    "            count_step.append(step)\n",
    "            break\n",
    "        \n",
    "        # Update using Q Learning and kNN\n",
    "        agent.TDUpdate(immediate_reward)\n",
    "        \n",
    "        # Prevent too long convergence        \n",
    "        if (step > 100000):\n",
    "            count_step.append(None)\n",
    "            break\n",
    "\n",
    "# Create table\n",
    "d = {\"Steps\": count_step}\n",
    "episode_table = pd.DataFrame(data=d, index=np.arange(1, len(count_step)+1))\n",
    "episode_table.index.names = ['Episodes']\n",
    "display(episode_table)\n",
    "\n",
    "# Create graph\n",
    "y = cumulative_average\n",
    "x = np.arange(1, len(cumulative_average) + 1)\n",
    "\n",
    "plt.scatter(x, y, color=\"orange\")\n",
    "plt.title(\"Average Rewards VS Steps\", fontsize = 16)\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "\n",
    "# Create line of best fit\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "plt.plot(x, a*x + b, '-')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PNA Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PNA** may be viewed as a refinement for kNN, with k adapting to the situation. On the one hand, it is beneficial to use large k since that means large data can be learn from. On the other hand, it is beneficial to learn only from the most similar past experiences (small k), as the data they provide should be the most relevant. \n",
    "\n",
    "PNA suggests that when predicting the value of an action a in a state s, k should be chosen dynamically to minimise:\n",
    "\n",
    "![equation](variance.jpg)\n",
    "\n",
    "where Var(Nsa) is the variance of observed rewards in the neighbourhood Nsa. This is a negative version of the term endorsing exploration in the UCB algorithm. Here it promotes choosing neighbourhoods that contain as much data as possible but with small variation between rewards. For example, in the ideal choice of k, all k nearest neighbours of (s, a) behave similarly, but actions farther away behave very differently. \n",
    "\n",
    "Action are chosen optimistically according to the UCB:\n",
    "\n",
    "![equation](action_selection.jpg)  \n",
    "\n",
    "with c > 0 a small constant. The upper confidence bound is composed of two terms: The first terms is the estimated value, and the second term is an exploration bonus for action whose value is uncertain. Actions can have uncertain value either because they have rarely been selected or have a high variance among previous returns. Meanwhile, the neighbourhoods are chosen \"pessimistically\" for each action to minimise the exploration bonus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PNAAgent:\n",
    "    \n",
    "    # Constructor\n",
    "    # Input: size of the storage for previous Q values, parameters for how many neighbours which the agent will choose\n",
    "    def __init__(self, size):\n",
    "        self.state = [0.0, -0.5]\n",
    "        self.actions = [-1, 0, 1]\n",
    "        self.q_storage = []\n",
    "        self.k = 0\n",
    "        self.alpha = 0.5 # choose fixed alpha, but we can varied alpha later\n",
    "        self.gamma = 1\n",
    "        \n",
    "        # Storage of the k nearest neighbour (data) and weight (inverse of distance) for a particular step\n",
    "        self.knn = []\n",
    "        self.weight = []\n",
    "        self.c = 0.5 # UCB selection constant\n",
    "        self.k_history = [] # used to store history of k chosen for each action\n",
    "        \n",
    "        # Initialise the storage with random point \n",
    "        for i in range(size):\n",
    "            initial_value = -1\n",
    "            initial_action = random.randint(-1, 1)\n",
    "            initial_state = [random.uniform(-0.07, 0.07), random.uniform(-1.2, 0.6)]\n",
    "            \n",
    "            # Each data on the array will consist of state, action pair + value\n",
    "            data = {\"state\": initial_state, \"value\": initial_value, \"action\": initial_action}\n",
    "            self.q_storage.append(data)\n",
    "    \n",
    "    # Find all index for a given value\n",
    "    # Input: value, list to search\n",
    "    # Output: list of all index where you find that value on the list\n",
    "    def findAllIndex(self, value, list_value):\n",
    "        indices = []\n",
    "        for i in range(len(list_value)):\n",
    "              if (value == list_value[i]):\n",
    "                    indices.append(i)\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    # Standardise feature vector given\n",
    "    # Input: feature vector to be standardised\n",
    "    # Output: standardised feature vector\n",
    "    def standardiseState(self, state):\n",
    "        standardised_state = []\n",
    "        standardised_velocity = 2 * ((state[0]+0.07) / (0.07+0.07)) - 1\n",
    "        standardised_position = 2 * ((state[1]+1.2) / (0.6+1.2)) - 1\n",
    "        standardised_state.append(standardised_velocity)\n",
    "        standardised_state.append(standardised_position)\n",
    "        \n",
    "        return(standardised_state)\n",
    "    \n",
    "    # Calculate Euclidean distance between 2 vectors\n",
    "    # Input: 2 feature vectors\n",
    "    # Output: distance between them\n",
    "    def calculateDistance(self, vector1, vector2):\n",
    "        return(math.sqrt((vector1[0]-vector2[0])**2 + (vector1[1]-vector2[1])**2))\n",
    "    \n",
    "    # Calculate total weight\n",
    "    # Input: list of weights\n",
    "    # Output: total weight\n",
    "    def calculateTotalWeight(self, weight_list):\n",
    "        total_weight = 0\n",
    "        for i in range(len(weight_list)):\n",
    "            total_weight += weight_list[i][2]\n",
    "        \n",
    "        return(total_weight)\n",
    "            \n",
    "    # Clear the knn list, k_history, and also the weight list\n",
    "    def cleanList(self):\n",
    "        self.knn = []\n",
    "        self.weight = []\n",
    "        self.k_history = []\n",
    "    \n",
    "    # Choose the appropriate k by minimising variance and maximising the number of data to learn\n",
    "    # Input: sorted neighbourhood list based on distance (distance, index, weight)\n",
    "    # Output: k (numbers of nearest neighbour) that minimise neighbourhood variance function\n",
    "    def chooseK(self, neighbourhood_list):\n",
    "        data_list = []\n",
    "        # Extract the data (Q value from the neighbourhood_list) and append it to the data_list\n",
    "        for data in neighbourhood_list:\n",
    "            data_list.append(self.q_storage[data[1]][\"value\"])\n",
    "            \n",
    "        # Initialise minimum variance\n",
    "        minimum_k = 2 # Variable that will be return that minimise the variance of the neighbourhood\n",
    "        minimum_var = self.neighbourhoodVariance(1, data_list[:2])\n",
    "        \n",
    "        # Iterate to find optimal k that will minimise the neighbourhood variance function\n",
    "        for i in range(3, len(neighbourhood_list)):\n",
    "            var = self.neighbourhoodVariance(1, data_list[:i])\n",
    "            k = i + 1\n",
    "            \n",
    "            # Update the k value and minimum var value if find parameter which better minimise than the previous value\n",
    "            if (var <= minimum_var):\n",
    "                minimum_k = k\n",
    "                minimum_var = var\n",
    "        \n",
    "        # Return the k which minimise neighbourhood variance function\n",
    "        return(minimum_k)\n",
    "    \n",
    "    # PNA variance function that needed to be minimise\n",
    "    # Input: constant c, list containing data points\n",
    "    # Output: calculation result from the neighbourhood variance function\n",
    "    def neighbourhoodVariance(self, c, data_list):\n",
    "        return(math.sqrt(c * np.var(data_list) / len(data_list)))\n",
    "    \n",
    "    # Get starting index for the weight list\n",
    "    # Input: action, k_history\n",
    "    # Output: starting index for the weight list\n",
    "    def getStartingIndex(self, action, k_history):\n",
    "        count_action = action + 1\n",
    "        if (count_action == 0):\n",
    "            return(0)\n",
    "        else:\n",
    "            index = 0\n",
    "            for i in range(count_action):\n",
    "                index += k_history[i]\n",
    "            return(index)\n",
    "        \n",
    "    # Apply the PNA algorithm for feature vector and store the data point on the neighbours array\n",
    "    # Input: feature vector of current state, actions array consisting of all possible actions, list that will store knn data and weights data, k_history\n",
    "    # Output: vector containing the value of taking each action (left, neutral, right)\n",
    "    def PNA(self, state, actions, knn_list, weight_list, k, k_history):\n",
    "        approximate_action = []\n",
    "        \n",
    "        # Get the standardised version of state\n",
    "        standardised_state = self.standardiseState(state)\n",
    "        \n",
    "        # Loop through every element in the storage array and only calculate for particular action\n",
    "        for action in actions:\n",
    "            temp = [] # array consisting of tuple (distance, original index, weight) for each point in the q_storage\n",
    "            for i in range(len(self.q_storage)):\n",
    "                data = self.q_storage[i]\n",
    "                # Only want to calculate the nearest neighbour state which has the same action\n",
    "                if (data[\"action\"] == action):\n",
    "                    vector_2 = data[\"state\"]\n",
    "                    standardised_vector_2 = self.standardiseState(vector_2)\n",
    "                    distance = self.calculateDistance(standardised_state, standardised_vector_2)\n",
    "                    index = i\n",
    "                    weight = 1 / (1+distance**2)\n",
    "            \n",
    "                    # Create the tuple and append that to temp\n",
    "                    temp.append(tuple((distance, index, weight)))\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            # After we finish looping through all of the point and calculating the standardise distance,\n",
    "            # Sort the tuple based on the distance and only take k of it and append that to the neighbours array\n",
    "            sorted_temp = sorted(temp, key=lambda x: x[0])\n",
    "            # Get the value of the k dynamically\n",
    "            k = self.chooseK(sorted_temp)\n",
    "            k_history.append(k)\n",
    "            \n",
    "            for i in range(k):\n",
    "                try:\n",
    "                    weight_list.append(sorted_temp[i])\n",
    "                    knn_list.append(self.q_storage[sorted_temp[i][1]])\n",
    "                except IndexError:\n",
    "                    sys.exit(0)\n",
    "            \n",
    "            # Calculate the expected value of the action and append it to the approximate_action array\n",
    "            expected_value = 0\n",
    "            # We also need to calculate the total weight to make it into valid probability that we can compute it's expectation\n",
    "            total_weight = self.calculateTotalWeight(weight_list[self.getStartingIndex(action, k_history):self.getStartingIndex(action, k_history)+k])\n",
    "            for i in range(self.getStartingIndex(action, k_history), self.getStartingIndex(action, k_history)+k):\n",
    "                try:\n",
    "                    weight = weight_list[i][2]\n",
    "                    probability = weight / total_weight\n",
    "                    expected_value += probability * knn_list[i][\"value\"]\n",
    "                except IndexError:\n",
    "                    sys.exit(0)\n",
    "                    \n",
    "            approximate_action.append(expected_value)\n",
    "        \n",
    "        return(approximate_action)\n",
    "    \n",
    "    # Calculate TD target based on Q Learning/ SARSAMAX\n",
    "    # Input: Immediate reward based on what the environment gave\n",
    "    # Output: TD target based on off policy Q learning\n",
    "    def calculateTDTarget(self, immediate_reward):\n",
    "        # Condition if final state\n",
    "        if (immediate_reward == 0):\n",
    "            return(0)\n",
    "        \n",
    "        k = 0\n",
    "        k_history = []\n",
    "        knn_prime = []\n",
    "        weight_prime = []\n",
    "        action_value = self.PNA(self.state, self.actions, knn_prime, weight_prime, k, k_history)\n",
    "        \n",
    "        return(immediate_reward + self.gamma*max(action_value))\n",
    "    \n",
    "    # Q learning TD updates on every neighbours on the kNN based on the contribution that are calculated using probability weight\n",
    "    # Input: Immediate reward based on what the environment gave\n",
    "    def TDUpdate(self, immediate_reward):\n",
    "        # First, calculate the TD target\n",
    "        td_target = self.calculateTDTarget(immediate_reward)\n",
    "        \n",
    "        # Iterate every kNN and update using Q learning method based on the weighting\n",
    "        total_weight = self.calculateTotalWeight(self.weight)\n",
    "        for i in range(len(self.weight)):\n",
    "            index = self.weight[i][1]\n",
    "            probability = self.weight[i][2] / total_weight\n",
    "            \n",
    "            # Begin updating\n",
    "            td_error = td_target - self.q_storage[index][\"value\"]\n",
    "            self.q_storage[index][\"value\"] = self.q_storage[index][\"value\"] + self.alpha*td_error*probability\n",
    "        \n",
    "        self.cleanList() # clean list to prepare for another step\n",
    "    \n",
    "    # Getting the maximum of the ucb method\n",
    "    # Input: action_value list, bonus_variance list\n",
    "    # Output: action which maximise\n",
    "    def maximumUCB(self, action_value, bonus_variance):\n",
    "        max_index = 0\n",
    "        max_value = action_value[0] + bonus_variance[0]\n",
    "        \n",
    "        for i in range(1, 3):\n",
    "            value = action_value[i] + bonus_variance[i]\n",
    "            \n",
    "            if (value >= max_value):\n",
    "                max_value = value\n",
    "                max_index = i\n",
    "        \n",
    "        return(max_index - 1) # return the action which maximise\n",
    "        \n",
    "    # Select which action to choose, whether left, neutral, or right (using UCB)\n",
    "    # Output: -1 (left), 0 (neutral), 1 (right)\n",
    "    def selectAction(self):\n",
    "        action_value = self.PNA(self.state, self.actions, self.knn, self.weight, self.k, self.k_history)\n",
    "        \n",
    "        # Second term of ucb, calculate the bonus variance\n",
    "        bonus_variance = []\n",
    "        start_index = [] # used to calculate start index for each action\n",
    "        finish_index = [] # used to calculate end index for each action\n",
    "        for action in self.actions:\n",
    "            data_list = []\n",
    "            # Prevent index out of bound\n",
    "            if (action != 1):\n",
    "                # Data extraction\n",
    "                start_index.append(self.getStartingIndex(action, self.k_history))\n",
    "                finish_index.append(self.getStartingIndex(action+1, self.k_history))\n",
    "                for i in range(self.getStartingIndex(action, self.k_history), self.getStartingIndex(action+1, self.k_history)):\n",
    "                    data_list.append(self.q_storage[self.weight[i][1]][\"value\"])\n",
    "                bonus_variance.append(self.neighbourhoodVariance(self.c, data_list))\n",
    "            else:\n",
    "                # Data extraction\n",
    "                start_index.append(self.k_history[2])\n",
    "                finish_index.append(len(self.weight))\n",
    "                for i in range(self.k_history[2], len(self.weight)):\n",
    "                    data_list.append(self.q_storage[self.weight[i][1]][\"value\"])\n",
    "                bonus_variance.append(self.neighbourhoodVariance(self.c, data_list))\n",
    "        \n",
    "        # Choose the action based on ucb method\n",
    "        action_chosen = self.maximumUCB(action_value, bonus_variance)\n",
    "                               \n",
    "        # Only store chosen data in the knn and weight list\n",
    "        # Clearance step\n",
    "        chosen_knn = []\n",
    "        chosen_weight = []\n",
    "        for i in range(start_index[action_chosen+1], finish_index[action_chosen+1]):\n",
    "            chosen_knn.append(self.knn[i])\n",
    "            chosen_weight.append(self.weight[i])\n",
    "        self.knn = chosen_knn\n",
    "        self.weight = chosen_weight\n",
    "        \n",
    "        return action_chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PNA Main Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PNA Main function** is responsible for initiating the PNA agent, environment and handling agent-environment interaction. It consists of a non-terminate inner loop that direct agent decision while also giving reward and next state from the environment. This inner loop will only break after the agent successfully get out of the environment, which in this case the mountain or if it is taking too long to converge. The outer loop can also be created to control the number of episodes which the agent will perform before the main function ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialise the environment and the agent\n",
    "size = 1000 # size of the q_storage \n",
    "agent = PNAAgent(size)\n",
    "mountain_car_environment = MountainCarEnvironment(agent)\n",
    "\n",
    "# Iterate the process, train the agent (training_iteration episodes)\n",
    "training_iteration = 1000\n",
    "for i in range(training_iteration):\n",
    "    step = 0\n",
    "    mountain_car_environment.reset()\n",
    "    while (True):\n",
    "        action = agent.selectAction()\n",
    "        step += 1\n",
    "        next_state = mountain_car_environment.nextState(action)\n",
    "    \n",
    "        # Change agent current state and getting reward\n",
    "        agent.state = next_state\n",
    "        immediate_reward = mountain_car_environment.calculateReward()\n",
    "    \n",
    "        # Test for successful learning\n",
    "        if (immediate_reward == 0):\n",
    "            print(\"Successfully get out of the mountain {} times. Steps taken for this episode: {}\".format(i + 1, step))\n",
    "            agent.TDUpdate(immediate_reward)\n",
    "            break\n",
    "        \n",
    "        # Update using Q Learning and kNN\n",
    "        agent.TDUpdate(immediate_reward)\n",
    "        \n",
    "        # Prevent too long convergence        \n",
    "        if (step > 100000):\n",
    "            print(\"Too many steps in this episode.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Function Approximation Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearAgent:\n",
    "    INITIAL_VELOCITY = 0.0\n",
    "    INITIAL_POSITION = -0.5\n",
    "    \n",
    "    REWARD_STEP = -1\n",
    "    REWARD_TERMINAL = 0\n",
    "    \n",
    "    ALPHA = 0.5\n",
    "    GAMMA = 1\n",
    "    EPSILON = 0.05\n",
    "    \n",
    "    ACTIONS = [-1, 0, 1]\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        self.state = [LinearAgent.INITIAL_VELOCITY, LinearAgent.INITIAL_POSITION] # capture agent current state\n",
    "        self.weight = [0, 0] # capture the weight vector that will be updated\n",
    "    \n",
    "    #\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
