{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pessimistic Neighbourhood Aggregation for States in Reinforcement Learning\n",
    "\n",
    "*Author: Maleakhi Agung Wijaya  \n",
    "Supervisors: Marcus Hutter, Sultan Javed Majeed  \n",
    "Date Created: 21/12/2017*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mountain Car Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mountain Car** is a standard testing domain in Reinforcement Learning, in which an under-powered car must drive up a steep hill. Since gravity is stronger than the car's engine, even at full throttle, the car cannot simply accelerate up the steep slope. The car is situated in a valley and must learn to leverage potential energy by driving up the opposite hill before the car is able to make it to the goal at the top of the rightmost hill.\n",
    "\n",
    "**Technical Details**\n",
    "- *State:* feature vectors consisting of velocity and position represented by an array [velocity, position]\n",
    "- *Reward:* -1 for every step taken, 0 for achieving the goal\n",
    "- *Action:* (left, neutral, right) represented by (-1, 0, 1)\n",
    "- *Initial state:* velocity = 0.0, position = -0.5 represented by [0.0, -0.5]\n",
    "- *Terminal state:* position >= 0.6\n",
    "- *Boundaries:* velocity = (-0.07, 0.07), position = (-1.2, 0.6)\n",
    "- *Update function:* velocity = velocity + (Action) \\* 0.001 + cos(3\\*Position) * (-0.0025), position = position + velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MountainCarEnvironment:\n",
    "    \"\"\"\n",
    "    Implementation of Sutton & Barto (1998) Mountain Car Problem environment.\n",
    "    \"\"\"\n",
    "    velocity_boundaries = (-0.07, 0.07)\n",
    "    position_boundaries = (-1.2, 0.6)  \n",
    "   \n",
    "    # Constructor for MountainCarEnvironment\n",
    "    # Input: agent for the MountainCarEnvironment\n",
    "    # Output: MountainCarEnvironment object\n",
    "    def __init__(self, car):\n",
    "        self.car = car\n",
    "        self.reset()\n",
    "        \n",
    "    # Compute next state (feature)\n",
    "    # Output: [new velocity, new position]\n",
    "    def nextState(self, action):\n",
    "        # Get current state (velocity, position) and the action chosen by the agent\n",
    "        velocity = self.car.state[0]\n",
    "        position = self.car.state[1]\n",
    "        \n",
    "        # Calculate the new velocity and new position\n",
    "        velocity += action * 0.001 + math.cos(3*position) * (-0.0025)\n",
    "        # Consider boundary for velocity\n",
    "        if (velocity < MountainCarEnvironment.velocity_boundaries[0]):\n",
    "            velocity = MountainCarEnvironment.velocity_boundaries[0]\n",
    "        elif (velocity > MountainCarEnvironment.velocity_boundaries[1]):\n",
    "            velocity = MountainCarEnvironment.velocity_boundaries[1]\n",
    "            \n",
    "        position += velocity\n",
    "        # Consider boundary for position\n",
    "        if (position < MountainCarEnvironment.position_boundaries[0]):\n",
    "            position = MountainCarEnvironment.position_boundaries[0]\n",
    "        elif (position > MountainCarEnvironment.position_boundaries[1]):\n",
    "            position = MountainCarEnvironment.position_boundaries[1]\n",
    "        \n",
    "        new_state = [velocity, position]\n",
    "        return(new_state)\n",
    "    \n",
    "    # Reset to the initial state    \n",
    "    def reset(self):\n",
    "        self.car.state[0] = 0.0\n",
    "        self.car.state[1] = -0.5\n",
    "        \n",
    "    # Give reward for each of the chosen action, depending on what the next state that the agent end up in\n",
    "    # Output: terminal state = 0, non-terminal state = -1\n",
    "    def calculateReward(self):\n",
    "        # Get current position of the agent\n",
    "        position = self.car.state[1]\n",
    "        \n",
    "        # Determine the reward given\n",
    "        if (position >= 0.6):\n",
    "            return(0)\n",
    "        else:\n",
    "            return(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## KNN-TD Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**kNN-TD** combines the concept of *K-Nearest Neighbours* and *TD-Learning* to learn and evaluate Q values in both continuous and discrete state space RL problems. This method is especially useful in continuous states RL problems as the number of (state, action) pairs is very large and thus impossible to store and learn this information. By choosing a particular k-values and decided some initial points over continuous states, one can estimate Q values based on calculated the weighted average of Q values of the k-nearest neighbours for the state that the agent are currently in and use that values to decide the next move using some decision methods (i.e. UCB or epsilon-greedy). As for the learning process, one can update all of the k-nearest neighbours that contribute for the Q calculation.\n",
    "\n",
    "**Algorithm:**\n",
    "1. Cover the whole state space by some initial Q(s,a) pairs, possibly scatter it uniformly across the whole state space and give an initial value of 0  \n",
    "2. When an agent in a particular state, get the feature vectors representing the state and possible actions from the state\n",
    "3. For each possible action from the state, calculate Q(s,a) pairs by taking the expected value from previous Q values based on k-nearest neighbours of a particular action.  \n",
    "*Steps for k-nearest neighbours:*\n",
    "    - Standardise every feature in the feature vectors to (-1, 1) or other ranges to make sure that 1 feature scaling not dominate the distance calculation\n",
    "    - Calculate the distance between current state and all of other points using distance formula (i.e. Euclidean distance) and store the k-nearest neighbours to knn vector, and it's distance\n",
    "    - Determine the weight (p(x)) for the expected value by using the inverse of the distance\n",
    "    - Estimate the Q(s,a) pairs using expectation formula using the weight and previous Q values of the kNN (average method)\n",
    "4. Using epsilon greedy decision method choose the next move\n",
    "5. Observe the reward and update the Q values for all of the neighbours using SARSA or Q Learning. (on the code below, I use Q Learning)\n",
    "6. Repeat step 2-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class KNNAgent:\n",
    "    \"\"\"\n",
    "    Implementation of agent (car) that will be used in the Mountain Car Environment using the kNN-TD underlying algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    # Constructor\n",
    "    # Input: size of the storage for previous Q values, parameters for how many neighbours which the agent will choose\n",
    "    def __init__(self, size, k):\n",
    "        self.state = [0.0, -0.5]\n",
    "        self.actions = [-1, 0, 1]\n",
    "        self.q_storage = []\n",
    "        self.k = k # fixed number of nearest neighbours that we will used\n",
    "        self.alpha = 0.5 # choose fixed alpha, but we can varied alpha later\n",
    "        self.gamma = 1\n",
    "        \n",
    "        # Storage of the k nearest neighbour (data) and weight (inverse of distance) for a particular step\n",
    "        self.knn = []\n",
    "        self.weight = []\n",
    "        \n",
    "        # Initialise the storage with random point \n",
    "        for i in range(size):\n",
    "            initial_value = -1\n",
    "            initial_action = random.randint(-1, 1)\n",
    "            initial_state = [random.uniform(-0.07, 0.07), random.uniform(-1.2, 0.6)]\n",
    "            \n",
    "            # Each data on the array will consist of state, action pair + value\n",
    "            data = {\"state\": initial_state, \"value\": initial_value, \"action\": initial_action}\n",
    "            self.q_storage.append(data)\n",
    "    \n",
    "    # Find all index for a given value\n",
    "    # Input: value, list to search\n",
    "    # Output: list of all index where you find that value on the list\n",
    "    def findAllIndex(self, value, list_value):\n",
    "        indices = []\n",
    "        for i in range(len(list_value)):\n",
    "              if (value == list_value[i]):\n",
    "                    indices.append(i)\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    # Standardise feature vector given\n",
    "    # Input: feature vector to be standardised\n",
    "    # Output: standardised feature vector\n",
    "    def standardiseState(self, state):\n",
    "        standardised_state = []\n",
    "        standardised_velocity = 2 * ((state[0]+0.07) / (0.07+0.07)) - 1\n",
    "        standardised_position = 2 * ((state[1]+1.2) / (0.6+1.2)) - 1\n",
    "        standardised_state.append(standardised_velocity)\n",
    "        standardised_state.append(standardised_position)\n",
    "        \n",
    "        return(standardised_state)\n",
    "    \n",
    "    # Calculate Euclidean distance between 2 vectors\n",
    "    # Input: 2 feature vectors\n",
    "    # Output: distance between them\n",
    "    def calculateDistance(self, vector1, vector2):\n",
    "        return(math.sqrt((vector1[0]-vector2[0])**2 + (vector1[1]-vector2[1])**2))\n",
    "    \n",
    "    # Calculate total weight\n",
    "    # Input: list of weights\n",
    "    # Output: total weight\n",
    "    def calculateTotalWeight(self, weight_list):\n",
    "        total_weight = 0\n",
    "        for i in range(len(weight_list)):\n",
    "            total_weight += weight_list[i][2]\n",
    "        \n",
    "        return(total_weight)\n",
    "    \n",
    "    # Apply the kNN algorithm for feature vector and store the data point on the neighbours array\n",
    "    # Input: feature vector of current state, actions array consisting of all possible actions, list that will store knn data and weights data\n",
    "    # Output: vector containing the value of taking each action (left, neutral, right)\n",
    "    def kNNTD(self, state, actions, knn_list, weight_list):\n",
    "        approximate_action = []\n",
    "        \n",
    "        # Get the standardised version of state\n",
    "        standardised_state = self.standardiseState(state)\n",
    "        \n",
    "        # Loop through every element in the storage array and only calculate for particular action\n",
    "        for action in actions:\n",
    "            temp = [] # array consisting of tuple (distance, original index, weight) for each point in the q_storage\n",
    "            for i in range(len(self.q_storage)):\n",
    "                data = self.q_storage[i]\n",
    "                # Only want to calculate the nearest neighbour state which has the same action\n",
    "                if (data[\"action\"] == action):\n",
    "                    vector_2 = data[\"state\"]\n",
    "                    standardised_vector_2 = self.standardiseState(vector_2)\n",
    "                    distance = self.calculateDistance(standardised_state, standardised_vector_2)\n",
    "                    index = i\n",
    "                    weight = 1 / (1+distance**2)\n",
    "            \n",
    "                    # Create the tuple and append that to temp\n",
    "                    temp.append(tuple((distance, index, weight)))\n",
    "                else:\n",
    "                    continue\n",
    "        \n",
    "            # After we finish looping through all of the point and calculating the standardise distance,\n",
    "            # Sort the tuple based on the distance and only take k of it and append that to the neighbours array\n",
    "            # We also need to calculate the total weight to make it into valid probability that we can compute it's expectation\n",
    "            sorted_temp = sorted(temp, key=lambda x: x[0])\n",
    "            for i in range(self.k):\n",
    "                try:\n",
    "                    weight_list.append(sorted_temp[i])\n",
    "                    knn_list.append(self.q_storage[sorted_temp[i][1]])\n",
    "                except IndexError:\n",
    "                    sys.exit(0)\n",
    "            \n",
    "            # Calculate the expected value of the action and append it to the approximate_action array\n",
    "            expected_value = 0\n",
    "            total_weight = self.calculateTotalWeight(weight_list[(action+1)*self.k:(action+1)*self.k + self.k])\n",
    "            for i in range((action+1)*self.k, (action+1)*self.k + self.k):\n",
    "                weight = weight_list[i][2]\n",
    "                probability = weight / total_weight\n",
    "                expected_value += probability * knn_list[i][\"value\"]\n",
    "                \n",
    "            approximate_action.append(expected_value)\n",
    "        \n",
    "        return(approximate_action)\n",
    "    \n",
    "    # Select which action to choose, whether left, neutral, or right (using epsilon greedy)\n",
    "    # Output: -1 (left), 0 (neutral), 1 (right)\n",
    "    def selectAction(self):\n",
    "        # First call the knn-td algorithm to determine the value of each Q(s,a) pairs\n",
    "        action_value = self.kNNTD(self.state, self.actions, self.knn, self.weight)\n",
    "        \n",
    "        # Use the epsilon-greedy method to choose value\n",
    "        epsilon = 0.1\n",
    "        random_number = random.uniform(0.0, 1.0)\n",
    "        if (random_number <= epsilon):\n",
    "            action_chosen = random.randint(-1, 1)\n",
    "        else:\n",
    "            # Return the action with highest Q(s,a)\n",
    "            possible_index = self.findAllIndex(max(action_value), action_value)\n",
    "            action_chosen = possible_index[random.randrange(len(possible_index))] - 1\n",
    "        \n",
    "        # Only store chosen data in the knn and weight list\n",
    "        # Clearance step\n",
    "        chosen_knn = []\n",
    "        chosen_weight = []\n",
    "        for i in range(self.k*(action_chosen+1), self.k*(action_chosen+1)+self.k):\n",
    "            chosen_knn.append(self.knn[i])\n",
    "            chosen_weight.append(self.weight[i])\n",
    "        self.knn = chosen_knn\n",
    "        self.weight = chosen_weight\n",
    "\n",
    "        return action_chosen\n",
    "    \n",
    "    # Calculate TD target based on Q Learning/ SARSAMAX\n",
    "    # Input: Immediate reward based on what the environment gave\n",
    "    # Output: TD target based on off policy Q learning\n",
    "    def calculateTDTarget(self, immediate_reward):\n",
    "        # Consider condition on the final state, return 0 immediately\n",
    "        if (immediate_reward == 0):\n",
    "            return(immediate_reward)\n",
    "        \n",
    "        knn_prime = []\n",
    "        weight_prime = []\n",
    "        action_value = self.kNNTD(self.state, self.actions, knn_prime, weight_prime)\n",
    "        \n",
    "        return(immediate_reward + self.gamma*max(action_value))\n",
    "    \n",
    "    # Q learning TD updates on every neighbours on the kNN based on the contribution that are calculated using probability weight\n",
    "    # Input: Immediate reward based on what the environment gave\n",
    "    def TDUpdate(self, immediate_reward):\n",
    "        # First, calculate the TD target\n",
    "        td_target = self.calculateTDTarget(immediate_reward)\n",
    "        \n",
    "        # Iterate every kNN and update using Q learning method based on the weighting\n",
    "        total_weight = self.calculateTotalWeight(self.weight)\n",
    "        for i in range(len(self.weight)):\n",
    "            index = self.weight[i][1]\n",
    "            probability = self.weight[i][2] / total_weight\n",
    "            \n",
    "            # Begin updating\n",
    "            td_error = td_target - self.q_storage[index][\"value\"]\n",
    "            self.q_storage[index][\"value\"] = self.q_storage[index][\"value\"] + self.alpha*td_error*probability\n",
    "        \n",
    "        self.cleanList() # clean list to prepare for another step\n",
    "            \n",
    "    # Clear the knn list and also the weight list\n",
    "    def cleanList(self):\n",
    "        self.knn = []\n",
    "        self.weight = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Main Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN Main function** is responsible for initiating the KNN agent, environment and handling agent-environment interaction. It consists of a non-terminate inner loop that direct agent decision while also giving reward and next state from the environment. This inner loop will only break after the agent successfully get out of the environment, which in this case the mountain or if it is taking too long to converge. The outer loop can also be created to control the number of episodes which the agent will perform before the main function ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully get out of the mountain 1 times. Steps taken for this episode: 11363\n",
      "Successfully get out of the mountain 2 times. Steps taken for this episode: 13631\n",
      "Successfully get out of the mountain 3 times. Steps taken for this episode: 3230\n",
      "Successfully get out of the mountain 4 times. Steps taken for this episode: 4730\n",
      "Successfully get out of the mountain 5 times. Steps taken for this episode: 2086\n",
      "Successfully get out of the mountain 6 times. Steps taken for this episode: 3285\n",
      "Successfully get out of the mountain 7 times. Steps taken for this episode: 6006\n",
      "Successfully get out of the mountain 8 times. Steps taken for this episode: 3685\n",
      "Successfully get out of the mountain 9 times. Steps taken for this episode: 6388\n",
      "Successfully get out of the mountain 10 times. Steps taken for this episode: 2712\n",
      "Successfully get out of the mountain 11 times. Steps taken for this episode: 3901\n",
      "Successfully get out of the mountain 12 times. Steps taken for this episode: 1088\n",
      "Successfully get out of the mountain 13 times. Steps taken for this episode: 7396\n",
      "Successfully get out of the mountain 14 times. Steps taken for this episode: 5177\n",
      "Successfully get out of the mountain 15 times. Steps taken for this episode: 6720\n",
      "Successfully get out of the mountain 16 times. Steps taken for this episode: 4216\n",
      "Successfully get out of the mountain 17 times. Steps taken for this episode: 3705\n",
      "Successfully get out of the mountain 18 times. Steps taken for this episode: 3570\n",
      "Successfully get out of the mountain 19 times. Steps taken for this episode: 2365\n",
      "Successfully get out of the mountain 20 times. Steps taken for this episode: 7330\n",
      "Successfully get out of the mountain 21 times. Steps taken for this episode: 6289\n",
      "Successfully get out of the mountain 22 times. Steps taken for this episode: 5750\n",
      "Successfully get out of the mountain 23 times. Steps taken for this episode: 6142\n",
      "Successfully get out of the mountain 24 times. Steps taken for this episode: 3958\n",
      "Successfully get out of the mountain 25 times. Steps taken for this episode: 3150\n",
      "Successfully get out of the mountain 26 times. Steps taken for this episode: 3798\n",
      "Successfully get out of the mountain 27 times. Steps taken for this episode: 3596\n",
      "Successfully get out of the mountain 28 times. Steps taken for this episode: 6353\n",
      "Successfully get out of the mountain 29 times. Steps taken for this episode: 5985\n",
      "Successfully get out of the mountain 30 times. Steps taken for this episode: 3987\n",
      "Successfully get out of the mountain 31 times. Steps taken for this episode: 3162\n",
      "Successfully get out of the mountain 32 times. Steps taken for this episode: 1804\n",
      "Successfully get out of the mountain 33 times. Steps taken for this episode: 3546\n",
      "Successfully get out of the mountain 34 times. Steps taken for this episode: 3168\n",
      "Successfully get out of the mountain 35 times. Steps taken for this episode: 4944\n",
      "Successfully get out of the mountain 36 times. Steps taken for this episode: 3662\n",
      "Successfully get out of the mountain 37 times. Steps taken for this episode: 4943\n",
      "Successfully get out of the mountain 38 times. Steps taken for this episode: 6285\n",
      "Successfully get out of the mountain 39 times. Steps taken for this episode: 3107\n",
      "Successfully get out of the mountain 40 times. Steps taken for this episode: 5091\n",
      "Successfully get out of the mountain 41 times. Steps taken for this episode: 4847\n",
      "Successfully get out of the mountain 42 times. Steps taken for this episode: 4930\n",
      "Successfully get out of the mountain 43 times. Steps taken for this episode: 7577\n",
      "Successfully get out of the mountain 44 times. Steps taken for this episode: 3043\n",
      "Successfully get out of the mountain 45 times. Steps taken for this episode: 3777\n",
      "Successfully get out of the mountain 46 times. Steps taken for this episode: 918\n",
      "Successfully get out of the mountain 47 times. Steps taken for this episode: 8297\n",
      "Successfully get out of the mountain 48 times. Steps taken for this episode: 10621\n",
      "Successfully get out of the mountain 49 times. Steps taken for this episode: 3917\n",
      "Successfully get out of the mountain 50 times. Steps taken for this episode: 4389\n",
      "Successfully get out of the mountain 51 times. Steps taken for this episode: 2861\n",
      "Successfully get out of the mountain 52 times. Steps taken for this episode: 7553\n",
      "Successfully get out of the mountain 53 times. Steps taken for this episode: 3103\n",
      "Successfully get out of the mountain 54 times. Steps taken for this episode: 3739\n",
      "Successfully get out of the mountain 55 times. Steps taken for this episode: 7247\n",
      "Successfully get out of the mountain 56 times. Steps taken for this episode: 18775\n",
      "Successfully get out of the mountain 57 times. Steps taken for this episode: 3738\n",
      "Successfully get out of the mountain 58 times. Steps taken for this episode: 2309\n",
      "Successfully get out of the mountain 59 times. Steps taken for this episode: 19815\n",
      "Successfully get out of the mountain 60 times. Steps taken for this episode: 5662\n",
      "Successfully get out of the mountain 61 times. Steps taken for this episode: 2275\n",
      "Successfully get out of the mountain 62 times. Steps taken for this episode: 2903\n",
      "Successfully get out of the mountain 63 times. Steps taken for this episode: 3179\n",
      "Successfully get out of the mountain 64 times. Steps taken for this episode: 2945\n",
      "Successfully get out of the mountain 65 times. Steps taken for this episode: 3546\n",
      "Successfully get out of the mountain 66 times. Steps taken for this episode: 586\n",
      "Successfully get out of the mountain 67 times. Steps taken for this episode: 3000\n",
      "Successfully get out of the mountain 68 times. Steps taken for this episode: 4117\n",
      "Successfully get out of the mountain 69 times. Steps taken for this episode: 3911\n",
      "Successfully get out of the mountain 70 times. Steps taken for this episode: 4238\n",
      "Successfully get out of the mountain 71 times. Steps taken for this episode: 2833\n",
      "Successfully get out of the mountain 72 times. Steps taken for this episode: 7895\n",
      "Successfully get out of the mountain 73 times. Steps taken for this episode: 1740\n",
      "Successfully get out of the mountain 74 times. Steps taken for this episode: 10820\n",
      "Successfully get out of the mountain 75 times. Steps taken for this episode: 6628\n",
      "Successfully get out of the mountain 76 times. Steps taken for this episode: 6466\n",
      "Successfully get out of the mountain 77 times. Steps taken for this episode: 14250\n",
      "Successfully get out of the mountain 78 times. Steps taken for this episode: 2159\n",
      "Successfully get out of the mountain 79 times. Steps taken for this episode: 4859\n",
      "Successfully get out of the mountain 80 times. Steps taken for this episode: 4505\n",
      "Successfully get out of the mountain 81 times. Steps taken for this episode: 450\n",
      "Successfully get out of the mountain 82 times. Steps taken for this episode: 4545\n",
      "Successfully get out of the mountain 83 times. Steps taken for this episode: 3027\n",
      "Successfully get out of the mountain 84 times. Steps taken for this episode: 7200\n",
      "Successfully get out of the mountain 85 times. Steps taken for this episode: 4052\n",
      "Successfully get out of the mountain 86 times. Steps taken for this episode: 5160\n",
      "Successfully get out of the mountain 87 times. Steps taken for this episode: 5118\n",
      "Successfully get out of the mountain 88 times. Steps taken for this episode: 8864\n",
      "Successfully get out of the mountain 89 times. Steps taken for this episode: 9536\n",
      "Successfully get out of the mountain 90 times. Steps taken for this episode: 4944\n",
      "Successfully get out of the mountain 91 times. Steps taken for this episode: 9530\n",
      "Successfully get out of the mountain 92 times. Steps taken for this episode: 3987\n",
      "Successfully get out of the mountain 93 times. Steps taken for this episode: 9783\n",
      "Successfully get out of the mountain 94 times. Steps taken for this episode: 2121\n",
      "Successfully get out of the mountain 95 times. Steps taken for this episode: 5443\n",
      "Successfully get out of the mountain 96 times. Steps taken for this episode: 3473\n",
      "Successfully get out of the mountain 97 times. Steps taken for this episode: 21532\n",
      "Successfully get out of the mountain 98 times. Steps taken for this episode: 4304\n",
      "Successfully get out of the mountain 99 times. Steps taken for this episode: 6658\n",
      "Successfully get out of the mountain 100 times. Steps taken for this episode: 4133\n",
      "Successfully get out of the mountain 101 times. Steps taken for this episode: 10255\n",
      "Successfully get out of the mountain 102 times. Steps taken for this episode: 9640\n",
      "Successfully get out of the mountain 103 times. Steps taken for this episode: 5759\n",
      "Successfully get out of the mountain 104 times. Steps taken for this episode: 12985\n",
      "Successfully get out of the mountain 105 times. Steps taken for this episode: 3166\n",
      "Successfully get out of the mountain 106 times. Steps taken for this episode: 308\n",
      "Successfully get out of the mountain 107 times. Steps taken for this episode: 10881\n",
      "Successfully get out of the mountain 108 times. Steps taken for this episode: 5442\n",
      "Successfully get out of the mountain 109 times. Steps taken for this episode: 11741\n",
      "Successfully get out of the mountain 110 times. Steps taken for this episode: 10656\n",
      "Successfully get out of the mountain 111 times. Steps taken for this episode: 2873\n",
      "Successfully get out of the mountain 112 times. Steps taken for this episode: 8049\n",
      "Successfully get out of the mountain 113 times. Steps taken for this episode: 10944\n",
      "Successfully get out of the mountain 114 times. Steps taken for this episode: 3070\n",
      "Successfully get out of the mountain 115 times. Steps taken for this episode: 11270\n",
      "Successfully get out of the mountain 116 times. Steps taken for this episode: 4437\n",
      "Successfully get out of the mountain 117 times. Steps taken for this episode: 12868\n",
      "Successfully get out of the mountain 118 times. Steps taken for this episode: 7125\n",
      "Successfully get out of the mountain 119 times. Steps taken for this episode: 3370\n",
      "Successfully get out of the mountain 120 times. Steps taken for this episode: 7213\n",
      "Successfully get out of the mountain 121 times. Steps taken for this episode: 4811\n",
      "Successfully get out of the mountain 122 times. Steps taken for this episode: 6240\n",
      "Successfully get out of the mountain 123 times. Steps taken for this episode: 10526\n",
      "Successfully get out of the mountain 124 times. Steps taken for this episode: 4349\n",
      "Successfully get out of the mountain 125 times. Steps taken for this episode: 505\n",
      "Successfully get out of the mountain 126 times. Steps taken for this episode: 7689\n",
      "Successfully get out of the mountain 127 times. Steps taken for this episode: 8055\n",
      "Successfully get out of the mountain 128 times. Steps taken for this episode: 4183\n",
      "Successfully get out of the mountain 129 times. Steps taken for this episode: 6164\n",
      "Successfully get out of the mountain 130 times. Steps taken for this episode: 2835\n",
      "Successfully get out of the mountain 131 times. Steps taken for this episode: 11487\n",
      "Successfully get out of the mountain 132 times. Steps taken for this episode: 4991\n",
      "Successfully get out of the mountain 133 times. Steps taken for this episode: 8700\n",
      "Successfully get out of the mountain 134 times. Steps taken for this episode: 10861\n",
      "Successfully get out of the mountain 135 times. Steps taken for this episode: 11368\n",
      "Successfully get out of the mountain 136 times. Steps taken for this episode: 5240\n",
      "Successfully get out of the mountain 137 times. Steps taken for this episode: 6850\n",
      "Successfully get out of the mountain 138 times. Steps taken for this episode: 10639\n",
      "Successfully get out of the mountain 139 times. Steps taken for this episode: 9200\n",
      "Successfully get out of the mountain 140 times. Steps taken for this episode: 10085\n",
      "Successfully get out of the mountain 141 times. Steps taken for this episode: 10163\n",
      "Successfully get out of the mountain 142 times. Steps taken for this episode: 5308\n",
      "Successfully get out of the mountain 143 times. Steps taken for this episode: 5209\n",
      "Successfully get out of the mountain 144 times. Steps taken for this episode: 4925\n",
      "Successfully get out of the mountain 145 times. Steps taken for this episode: 463\n",
      "Successfully get out of the mountain 146 times. Steps taken for this episode: 821\n",
      "Successfully get out of the mountain 147 times. Steps taken for this episode: 6795\n",
      "Successfully get out of the mountain 148 times. Steps taken for this episode: 4653\n",
      "Successfully get out of the mountain 149 times. Steps taken for this episode: 13162\n",
      "Successfully get out of the mountain 150 times. Steps taken for this episode: 3356\n",
      "Successfully get out of the mountain 151 times. Steps taken for this episode: 5337\n",
      "Successfully get out of the mountain 152 times. Steps taken for this episode: 226\n",
      "Successfully get out of the mountain 153 times. Steps taken for this episode: 417\n",
      "Successfully get out of the mountain 154 times. Steps taken for this episode: 524\n",
      "Successfully get out of the mountain 155 times. Steps taken for this episode: 654\n",
      "Successfully get out of the mountain 156 times. Steps taken for this episode: 855\n",
      "Successfully get out of the mountain 157 times. Steps taken for this episode: 960\n",
      "Successfully get out of the mountain 158 times. Steps taken for this episode: 867\n",
      "Successfully get out of the mountain 159 times. Steps taken for this episode: 8199\n",
      "Successfully get out of the mountain 160 times. Steps taken for this episode: 2628\n",
      "Successfully get out of the mountain 161 times. Steps taken for this episode: 7414\n",
      "Successfully get out of the mountain 162 times. Steps taken for this episode: 4687\n",
      "Successfully get out of the mountain 163 times. Steps taken for this episode: 2694\n",
      "Successfully get out of the mountain 164 times. Steps taken for this episode: 14523\n",
      "Successfully get out of the mountain 165 times. Steps taken for this episode: 6915\n",
      "Successfully get out of the mountain 166 times. Steps taken for this episode: 1156\n",
      "Successfully get out of the mountain 167 times. Steps taken for this episode: 10658\n",
      "Successfully get out of the mountain 168 times. Steps taken for this episode: 2958\n",
      "Successfully get out of the mountain 169 times. Steps taken for this episode: 9169\n",
      "Successfully get out of the mountain 170 times. Steps taken for this episode: 7903\n",
      "Successfully get out of the mountain 171 times. Steps taken for this episode: 14331\n",
      "Successfully get out of the mountain 172 times. Steps taken for this episode: 14757\n",
      "Successfully get out of the mountain 173 times. Steps taken for this episode: 9751\n",
      "Successfully get out of the mountain 174 times. Steps taken for this episode: 4479\n",
      "Successfully get out of the mountain 175 times. Steps taken for this episode: 3896\n",
      "Successfully get out of the mountain 176 times. Steps taken for this episode: 10096\n",
      "Successfully get out of the mountain 177 times. Steps taken for this episode: 8897\n",
      "Successfully get out of the mountain 178 times. Steps taken for this episode: 9635\n",
      "Successfully get out of the mountain 179 times. Steps taken for this episode: 4228\n",
      "Successfully get out of the mountain 180 times. Steps taken for this episode: 10746\n",
      "Successfully get out of the mountain 181 times. Steps taken for this episode: 6033\n",
      "Successfully get out of the mountain 182 times. Steps taken for this episode: 12708\n",
      "Successfully get out of the mountain 183 times. Steps taken for this episode: 13405\n",
      "Successfully get out of the mountain 184 times. Steps taken for this episode: 5176\n",
      "Successfully get out of the mountain 185 times. Steps taken for this episode: 13046\n",
      "Successfully get out of the mountain 186 times. Steps taken for this episode: 301\n",
      "Successfully get out of the mountain 187 times. Steps taken for this episode: 3495\n",
      "Successfully get out of the mountain 188 times. Steps taken for this episode: 2058\n",
      "Successfully get out of the mountain 189 times. Steps taken for this episode: 11403\n",
      "Successfully get out of the mountain 190 times. Steps taken for this episode: 29528\n",
      "Successfully get out of the mountain 191 times. Steps taken for this episode: 6442\n",
      "Successfully get out of the mountain 192 times. Steps taken for this episode: 1282\n",
      "Successfully get out of the mountain 193 times. Steps taken for this episode: 8620\n",
      "Successfully get out of the mountain 194 times. Steps taken for this episode: 7409\n",
      "Successfully get out of the mountain 195 times. Steps taken for this episode: 6141\n",
      "Successfully get out of the mountain 196 times. Steps taken for this episode: 14635\n",
      "Successfully get out of the mountain 197 times. Steps taken for this episode: 4181\n",
      "Successfully get out of the mountain 198 times. Steps taken for this episode: 667\n",
      "Successfully get out of the mountain 199 times. Steps taken for this episode: 1056\n",
      "Successfully get out of the mountain 200 times. Steps taken for this episode: 9509\n",
      "Successfully get out of the mountain 201 times. Steps taken for this episode: 1125\n",
      "Successfully get out of the mountain 202 times. Steps taken for this episode: 1758\n",
      "Successfully get out of the mountain 203 times. Steps taken for this episode: 11951\n",
      "Successfully get out of the mountain 204 times. Steps taken for this episode: 2053\n",
      "Successfully get out of the mountain 205 times. Steps taken for this episode: 10410\n",
      "Successfully get out of the mountain 206 times. Steps taken for this episode: 14117\n",
      "Successfully get out of the mountain 207 times. Steps taken for this episode: 1828\n",
      "Successfully get out of the mountain 208 times. Steps taken for this episode: 3274\n",
      "Successfully get out of the mountain 209 times. Steps taken for this episode: 5723\n",
      "Successfully get out of the mountain 210 times. Steps taken for this episode: 6220\n",
      "Successfully get out of the mountain 211 times. Steps taken for this episode: 12297\n",
      "Successfully get out of the mountain 212 times. Steps taken for this episode: 11511\n",
      "Successfully get out of the mountain 213 times. Steps taken for this episode: 13669\n",
      "Successfully get out of the mountain 214 times. Steps taken for this episode: 12964\n",
      "Successfully get out of the mountain 215 times. Steps taken for this episode: 11374\n",
      "Successfully get out of the mountain 216 times. Steps taken for this episode: 13921\n",
      "Successfully get out of the mountain 217 times. Steps taken for this episode: 10644\n",
      "Successfully get out of the mountain 218 times. Steps taken for this episode: 14719\n",
      "Successfully get out of the mountain 219 times. Steps taken for this episode: 343\n",
      "Successfully get out of the mountain 220 times. Steps taken for this episode: 719\n",
      "Successfully get out of the mountain 221 times. Steps taken for this episode: 774\n",
      "Successfully get out of the mountain 222 times. Steps taken for this episode: 1352\n",
      "Successfully get out of the mountain 223 times. Steps taken for this episode: 9235\n",
      "Successfully get out of the mountain 224 times. Steps taken for this episode: 538\n",
      "Successfully get out of the mountain 225 times. Steps taken for this episode: 8082\n",
      "Successfully get out of the mountain 226 times. Steps taken for this episode: 3916\n",
      "Successfully get out of the mountain 227 times. Steps taken for this episode: 3271\n",
      "Successfully get out of the mountain 228 times. Steps taken for this episode: 15475\n",
      "Successfully get out of the mountain 229 times. Steps taken for this episode: 3238\n",
      "Successfully get out of the mountain 230 times. Steps taken for this episode: 4430\n",
      "Successfully get out of the mountain 231 times. Steps taken for this episode: 12855\n",
      "Successfully get out of the mountain 232 times. Steps taken for this episode: 5505\n",
      "Successfully get out of the mountain 233 times. Steps taken for this episode: 8449\n",
      "Successfully get out of the mountain 234 times. Steps taken for this episode: 11296\n",
      "Successfully get out of the mountain 235 times. Steps taken for this episode: 4636\n",
      "Successfully get out of the mountain 236 times. Steps taken for this episode: 15705\n",
      "Successfully get out of the mountain 237 times. Steps taken for this episode: 12188\n",
      "Successfully get out of the mountain 238 times. Steps taken for this episode: 7331\n",
      "Successfully get out of the mountain 239 times. Steps taken for this episode: 10268\n",
      "Successfully get out of the mountain 240 times. Steps taken for this episode: 4143\n",
      "Successfully get out of the mountain 241 times. Steps taken for this episode: 346\n",
      "Successfully get out of the mountain 242 times. Steps taken for this episode: 9322\n",
      "Successfully get out of the mountain 243 times. Steps taken for this episode: 2053\n",
      "Successfully get out of the mountain 244 times. Steps taken for this episode: 12360\n",
      "Successfully get out of the mountain 245 times. Steps taken for this episode: 11526\n",
      "Successfully get out of the mountain 246 times. Steps taken for this episode: 12511\n",
      "Successfully get out of the mountain 247 times. Steps taken for this episode: 5649\n",
      "Successfully get out of the mountain 248 times. Steps taken for this episode: 6789\n",
      "Successfully get out of the mountain 249 times. Steps taken for this episode: 16327\n",
      "Successfully get out of the mountain 250 times. Steps taken for this episode: 8036\n",
      "Successfully get out of the mountain 251 times. Steps taken for this episode: 11756\n",
      "Successfully get out of the mountain 252 times. Steps taken for this episode: 20434\n",
      "Successfully get out of the mountain 253 times. Steps taken for this episode: 14058\n",
      "Successfully get out of the mountain 254 times. Steps taken for this episode: 13138\n",
      "Successfully get out of the mountain 255 times. Steps taken for this episode: 13587\n",
      "Successfully get out of the mountain 256 times. Steps taken for this episode: 17079\n",
      "Successfully get out of the mountain 257 times. Steps taken for this episode: 13988\n",
      "Successfully get out of the mountain 258 times. Steps taken for this episode: 2370\n",
      "Successfully get out of the mountain 259 times. Steps taken for this episode: 10289\n",
      "Successfully get out of the mountain 260 times. Steps taken for this episode: 15025\n",
      "Successfully get out of the mountain 261 times. Steps taken for this episode: 5767\n",
      "Successfully get out of the mountain 262 times. Steps taken for this episode: 11497\n",
      "Successfully get out of the mountain 263 times. Steps taken for this episode: 11266\n",
      "Successfully get out of the mountain 264 times. Steps taken for this episode: 14319\n",
      "Successfully get out of the mountain 265 times. Steps taken for this episode: 443\n",
      "Successfully get out of the mountain 266 times. Steps taken for this episode: 14772\n",
      "Successfully get out of the mountain 267 times. Steps taken for this episode: 14296\n",
      "Successfully get out of the mountain 268 times. Steps taken for this episode: 5536\n",
      "Successfully get out of the mountain 269 times. Steps taken for this episode: 14642\n",
      "Successfully get out of the mountain 270 times. Steps taken for this episode: 19510\n",
      "Successfully get out of the mountain 271 times. Steps taken for this episode: 3332\n",
      "Successfully get out of the mountain 272 times. Steps taken for this episode: 17190\n",
      "Successfully get out of the mountain 273 times. Steps taken for this episode: 10502\n",
      "Successfully get out of the mountain 274 times. Steps taken for this episode: 16564\n",
      "Successfully get out of the mountain 275 times. Steps taken for this episode: 16009\n",
      "Successfully get out of the mountain 276 times. Steps taken for this episode: 12415\n",
      "Successfully get out of the mountain 277 times. Steps taken for this episode: 11085\n",
      "Successfully get out of the mountain 278 times. Steps taken for this episode: 5331\n",
      "Successfully get out of the mountain 279 times. Steps taken for this episode: 13636\n",
      "Successfully get out of the mountain 280 times. Steps taken for this episode: 15840\n",
      "Successfully get out of the mountain 281 times. Steps taken for this episode: 3915\n",
      "Successfully get out of the mountain 282 times. Steps taken for this episode: 14239\n",
      "Successfully get out of the mountain 283 times. Steps taken for this episode: 11996\n",
      "Successfully get out of the mountain 284 times. Steps taken for this episode: 3026\n",
      "Successfully get out of the mountain 285 times. Steps taken for this episode: 9600\n",
      "Successfully get out of the mountain 286 times. Steps taken for this episode: 15445\n",
      "Successfully get out of the mountain 287 times. Steps taken for this episode: 6893\n",
      "Successfully get out of the mountain 288 times. Steps taken for this episode: 1814\n",
      "Successfully get out of the mountain 289 times. Steps taken for this episode: 15639\n",
      "Successfully get out of the mountain 290 times. Steps taken for this episode: 3807\n",
      "Successfully get out of the mountain 291 times. Steps taken for this episode: 12658\n",
      "Successfully get out of the mountain 292 times. Steps taken for this episode: 15509\n",
      "Successfully get out of the mountain 293 times. Steps taken for this episode: 12637\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5d0c40bbd2b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmountain_car_environment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselectAction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mstep\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmountain_car_environment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnextState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-9c39e41c24d3>\u001b[0m in \u001b[0;36mselectAction\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mselectAction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[1;31m# First call the knn-td algorithm to determine the value of each Q(s,a) pairs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0maction_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkNNTD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mknn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[1;31m# Use the epsilon-greedy method to choose value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-9c39e41c24d3>\u001b[0m in \u001b[0;36mkNNTD\u001b[0;34m(self, state, actions, knn_list, weight_list)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[1;31m# Create the tuple and append that to temp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                     \u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main Function script to test kNN agent\n",
    "\"\"\"\n",
    "\n",
    "# Initialise the environment and the agent\n",
    "size = 1000 # size of the q_storage \n",
    "k = 50 # knn parameter\n",
    "agent = KNNAgent(size, k)\n",
    "mountain_car_environment = MountainCarEnvironment(agent)\n",
    "\n",
    "# Iterate the process, train the agent (training_iteration episodes)\n",
    "training_iteration = 1000\n",
    "for i in range(training_iteration):\n",
    "    step = 0\n",
    "    mountain_car_environment.reset()\n",
    "    while (True):\n",
    "        action = agent.selectAction()\n",
    "        step += 1\n",
    "        next_state = mountain_car_environment.nextState(action)\n",
    "    \n",
    "        # Change agent current state and getting reward\n",
    "        agent.state = next_state\n",
    "        immediate_reward = mountain_car_environment.calculateReward()\n",
    "    \n",
    "        # Test for successful learning\n",
    "        if (immediate_reward == 0):\n",
    "            print(\"Successfully get out of the mountain {} times. Steps taken for this episode: {}\".format(i + 1, step))\n",
    "            agent.TDUpdate(immediate_reward)\n",
    "            break\n",
    "        \n",
    "        # Update using Q Learning and kNN\n",
    "        agent.TDUpdate(immediate_reward)\n",
    "        \n",
    "        # Prevent too long convergence        \n",
    "        if (step > 100000):\n",
    "            print(\"Too many steps in this episode.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PNA Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PNA** may be viewed as a refinement for kNN, with k adapting to the situation. On the one hand, it is beneficial to use large k since that means large data can be learn from. On the other hand, it is beneficial to learn only from the most similar past experiences (small k), as the data they provide should be the most relevant.\n",
    "\n",
    "PNA suggests that when predicting the value of an action a in a state s, k should be chosen dynamically to minimise\n",
    "\n",
    "![equation](variance.jpg)\n",
    "\n",
    "where Var(Nsa) is the variance of observed rewards in the neighbourhood Nsa. This is a negative version of the term endorsing exploration in the UCB algorithm. Here it promotes choosing neighbourhoods that contain as much data as possible but with small variation between rewards. For example, in the ideal choice of k, all k nearest neighbours of (s, a) behave similarly, but actions farther away behave very differently.\n",
    "\n",
    "Action are chosen optimistically according to the UCB\n",
    "\n",
    "![equation](action_selection.jpg)  \n",
    "\n",
    "with c > 0 a small constant. The upper confidence bound is composed of two terms: The first terms is the estimated value, and the second term is an exploration bonus for action whose value is uncertain. Actions can have uncertain value either because they have rarely been selected or have a high variance among previous returns. Meanwhile, the neighbourhoods are chosen \"pessimistically\" for each action to minimise the exploration bonus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PNAAgent:\n",
    "    \"\"\"\n",
    "    Implementation of agent (car) that will be used in the Mountain Car Environment using the PNA underlying algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    # Constructor\n",
    "    # Input: size of the storage for previous Q values, parameters for how many neighbours which the agent will choose\n",
    "    def __init__(self, size):\n",
    "        self.state = [0.0, -0.5]\n",
    "        self.actions = [-1, 0, 1]\n",
    "        self.q_storage = []\n",
    "        self.k = 0\n",
    "        self.alpha = 0.5 # choose fixed alpha, but we can varied alpha later\n",
    "        self.gamma = 1\n",
    "        \n",
    "        # Storage of the k nearest neighbour (data) and weight (inverse of distance) for a particular step\n",
    "        self.knn = []\n",
    "        self.weight = []\n",
    "        self.c = 0.5 # UCB selection constant\n",
    "        self.k_history = [] # used to store history of k chosen for each action\n",
    "        \n",
    "        # Initialise the storage with random point \n",
    "        for i in range(size):\n",
    "            initial_value = -1\n",
    "            initial_action = random.randint(-1, 1)\n",
    "            initial_state = [random.uniform(-0.07, 0.07), random.uniform(-1.2, 0.6)]\n",
    "            \n",
    "            # Each data on the array will consist of state, action pair + value\n",
    "            data = {\"state\": initial_state, \"value\": initial_value, \"action\": initial_action}\n",
    "            self.q_storage.append(data)\n",
    "    \n",
    "    # Find all index for a given value\n",
    "    # Input: value, list to search\n",
    "    # Output: list of all index where you find that value on the list\n",
    "    def findAllIndex(self, value, list_value):\n",
    "        indices = []\n",
    "        for i in range(len(list_value)):\n",
    "              if (value == list_value[i]):\n",
    "                    indices.append(i)\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    # Standardise feature vector given\n",
    "    # Input: feature vector to be standardised\n",
    "    # Output: standardised feature vector\n",
    "    def standardiseState(self, state):\n",
    "        standardised_state = []\n",
    "        standardised_velocity = 2 * ((state[0]+0.07) / (0.07+0.07)) - 1\n",
    "        standardised_position = 2 * ((state[1]+1.2) / (0.6+1.2)) - 1\n",
    "        standardised_state.append(standardised_velocity)\n",
    "        standardised_state.append(standardised_position)\n",
    "        \n",
    "        return(standardised_state)\n",
    "    \n",
    "    # Calculate Euclidean distance between 2 vectors\n",
    "    # Input: 2 feature vectors\n",
    "    # Output: distance between them\n",
    "    def calculateDistance(self, vector1, vector2):\n",
    "        return(math.sqrt((vector1[0]-vector2[0])**2 + (vector1[1]-vector2[1])**2))\n",
    "    \n",
    "    # Calculate total weight\n",
    "    # Input: list of weights\n",
    "    # Output: total weight\n",
    "    def calculateTotalWeight(self, weight_list):\n",
    "        total_weight = 0\n",
    "        for i in range(len(weight_list)):\n",
    "            total_weight += weight_list[i][2]\n",
    "        \n",
    "        return(total_weight)\n",
    "            \n",
    "    # Clear the knn list, k_history, and also the weight list\n",
    "    def cleanList(self):\n",
    "        self.knn = []\n",
    "        self.weight = []\n",
    "        self.k_history = []\n",
    "    \n",
    "    # Choose the appropriate k by minimising variance and maximising the number of data to learn\n",
    "    # Input: sorted neighbourhood list based on distance (distance, index, weight)\n",
    "    # Output: k (numbers of nearest neighbour) that minimise neighbourhood variance function\n",
    "    def chooseK(self, neighbourhood_list):\n",
    "        data_list = []\n",
    "        # Extract the data (Q value from the neighbourhood_list) and append it to the data_list\n",
    "        for data in neighbourhood_list:\n",
    "            data_list.append(self.q_storage[data[1]][\"value\"])\n",
    "            \n",
    "        # Initialise minimum variance\n",
    "        minimum_k = 2 # Variable that will be return that minimise the variance of the neighbourhood\n",
    "        minimum_var = self.neighbourhoodVariance(1, data_list[:2])\n",
    "        \n",
    "        # Iterate to find optimal k that will minimise the neighbourhood variance function\n",
    "        for i in range(3, len(neighbourhood_list)):\n",
    "            var = self.neighbourhoodVariance(1, data_list[:i])\n",
    "            k = i + 1\n",
    "            \n",
    "            # Update the k value and minimum var value if find parameter which better minimise than the previous value\n",
    "            if (var <= minimum_var):\n",
    "                minimum_k = k\n",
    "                minimum_var = var\n",
    "        \n",
    "        # Return the k which minimise neighbourhood variance function\n",
    "        return(minimum_k)\n",
    "    \n",
    "    # PNA variance function that needed to be minimise\n",
    "    # Input: constant c, list containing data points\n",
    "    # Output: calculation result from the neighbourhood variance function\n",
    "    def neighbourhoodVariance(self, c, data_list):\n",
    "        return(math.sqrt(c * np.var(data_list) / len(data_list)))\n",
    "    \n",
    "    # Get starting index for the weight list\n",
    "    # Input: action, k_history\n",
    "    # Output: starting index for the weight list\n",
    "    def getStartingIndex(self, action, k_history):\n",
    "        count_action = action + 1\n",
    "        if (count_action == 0):\n",
    "            return(0)\n",
    "        else:\n",
    "            index = 0\n",
    "            for i in range(count_action):\n",
    "                index += k_history[i]\n",
    "            return(index)\n",
    "        \n",
    "    # Apply the PNA algorithm for feature vector and store the data point on the neighbours array\n",
    "    # Input: feature vector of current state, actions array consisting of all possible actions, list that will store knn data and weights data, k_history\n",
    "    # Output: vector containing the value of taking each action (left, neutral, right)\n",
    "    def PNA(self, state, actions, knn_list, weight_list, k, k_history):\n",
    "        approximate_action = []\n",
    "        \n",
    "        # Get the standardised version of state\n",
    "        standardised_state = self.standardiseState(state)\n",
    "        \n",
    "        # Loop through every element in the storage array and only calculate for particular action\n",
    "        for action in actions:\n",
    "            temp = [] # array consisting of tuple (distance, original index, weight) for each point in the q_storage\n",
    "            for i in range(len(self.q_storage)):\n",
    "                data = self.q_storage[i]\n",
    "                # Only want to calculate the nearest neighbour state which has the same action\n",
    "                if (data[\"action\"] == action):\n",
    "                    vector_2 = data[\"state\"]\n",
    "                    standardised_vector_2 = self.standardiseState(vector_2)\n",
    "                    distance = self.calculateDistance(standardised_state, standardised_vector_2)\n",
    "                    index = i\n",
    "                    weight = 1 / (1+distance**2)\n",
    "            \n",
    "                    # Create the tuple and append that to temp\n",
    "                    temp.append(tuple((distance, index, weight)))\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            # After we finish looping through all of the point and calculating the standardise distance,\n",
    "            # Sort the tuple based on the distance and only take k of it and append that to the neighbours array\n",
    "            sorted_temp = sorted(temp, key=lambda x: x[0])\n",
    "            # Get the value of the k dynamically\n",
    "            k = self.chooseK(sorted_temp)\n",
    "            k_history.append(k)\n",
    "            \n",
    "            for i in range(k):\n",
    "                try:\n",
    "                    weight_list.append(sorted_temp[i])\n",
    "                    knn_list.append(self.q_storage[sorted_temp[i][1]])\n",
    "                except IndexError:\n",
    "                    sys.exit(0)\n",
    "            \n",
    "            # Calculate the expected value of the action and append it to the approximate_action array\n",
    "            expected_value = 0\n",
    "            # We also need to calculate the total weight to make it into valid probability that we can compute it's expectation\n",
    "            total_weight = self.calculateTotalWeight(weight_list[self.getStartingIndex(action, k_history):self.getStartingIndex(action, k_history)+k])\n",
    "            for i in range(self.getStartingIndex(action, k_history), self.getStartingIndex(action, k_history)+k):\n",
    "                try:\n",
    "                    weight = weight_list[i][2]\n",
    "                    probability = weight / total_weight\n",
    "                    expected_value += probability * knn_list[i][\"value\"]\n",
    "                except IndexError:\n",
    "                    sys.exit(0)\n",
    "                    \n",
    "            approximate_action.append(expected_value)\n",
    "        \n",
    "        return(approximate_action)\n",
    "    \n",
    "    # Calculate TD target based on Q Learning/ SARSAMAX\n",
    "    # Input: Immediate reward based on what the environment gave\n",
    "    # Output: TD target based on off policy Q learning\n",
    "    def calculateTDTarget(self, immediate_reward):\n",
    "        # Condition if final state\n",
    "        if (immediate_reward == 0):\n",
    "            return(0)\n",
    "        \n",
    "        k = 0\n",
    "        k_history = []\n",
    "        knn_prime = []\n",
    "        weight_prime = []\n",
    "        action_value = self.PNA(self.state, self.actions, knn_prime, weight_prime, k, k_history)\n",
    "        \n",
    "        return(immediate_reward + self.gamma*max(action_value))\n",
    "    \n",
    "    # Q learning TD updates on every neighbours on the kNN based on the contribution that are calculated using probability weight\n",
    "    # Input: Immediate reward based on what the environment gave\n",
    "    def TDUpdate(self, immediate_reward):\n",
    "        # First, calculate the TD target\n",
    "        td_target = self.calculateTDTarget(immediate_reward)\n",
    "        \n",
    "        # Iterate every kNN and update using Q learning method based on the weighting\n",
    "        total_weight = self.calculateTotalWeight(self.weight)\n",
    "        for i in range(len(self.weight)):\n",
    "            index = self.weight[i][1]\n",
    "            probability = self.weight[i][2] / total_weight\n",
    "            \n",
    "            # Begin updating\n",
    "            td_error = td_target - self.q_storage[index][\"value\"]\n",
    "            self.q_storage[index][\"value\"] = self.q_storage[index][\"value\"] + self.alpha*td_error*probability\n",
    "        \n",
    "        self.cleanList() # clean list to prepare for another step\n",
    "    \n",
    "    # Getting the maximum of the ucb method\n",
    "    # Input: action_value list, bonus_variance list\n",
    "    # Output: action which maximise\n",
    "    def maximumUCB(self, action_value, bonus_variance):\n",
    "        max_index = 0\n",
    "        max_value = action_value[0] + bonus_variance[0]\n",
    "        \n",
    "        for i in range(1, 3):\n",
    "            value = action_value[i] + bonus_variance[i]\n",
    "            \n",
    "            if (value >= max_value):\n",
    "                max_value = value\n",
    "                max_index = i\n",
    "        \n",
    "        return(max_index - 1) # return the action which maximise\n",
    "        \n",
    "    # Select which action to choose, whether left, neutral, or right (using UCB)\n",
    "    # Output: -1 (left), 0 (neutral), 1 (right)\n",
    "    def selectAction(self):\n",
    "        action_value = self.PNA(self.state, self.actions, self.knn, self.weight, self.k, self.k_history)\n",
    "        \n",
    "        # Second term of ucb, calculate the bonus variance\n",
    "        bonus_variance = []\n",
    "        start_index = [] # used to calculate start index for each action\n",
    "        finish_index = [] # used to calculate end index for each action\n",
    "        for action in self.actions:\n",
    "            data_list = []\n",
    "            # Prevent index out of bound\n",
    "            if (action != 1):\n",
    "                # Data extraction\n",
    "                start_index.append(self.getStartingIndex(action, self.k_history))\n",
    "                finish_index.append(self.getStartingIndex(action+1, self.k_history))\n",
    "                for i in range(self.getStartingIndex(action, self.k_history), self.getStartingIndex(action+1, self.k_history)):\n",
    "                    data_list.append(self.q_storage[self.weight[i][1]][\"value\"])\n",
    "                bonus_variance.append(self.neighbourhoodVariance(self.c, data_list))\n",
    "            else:\n",
    "                # Data extraction\n",
    "                start_index.append(self.k_history[2])\n",
    "                finish_index.append(len(self.weight))\n",
    "                for i in range(self.k_history[2], len(self.weight)):\n",
    "                    data_list.append(self.q_storage[self.weight[i][1]][\"value\"])\n",
    "                bonus_variance.append(self.neighbourhoodVariance(self.c, data_list))\n",
    "        \n",
    "        # Choose the action based on ucb method\n",
    "        action_chosen = self.maximumUCB(action_value, bonus_variance)\n",
    "                               \n",
    "        # Only store chosen data in the knn and weight list\n",
    "        # Clearance step\n",
    "        chosen_knn = []\n",
    "        chosen_weight = []\n",
    "        for i in range(start_index[action_chosen+1], finish_index[action_chosen+1]):\n",
    "            chosen_knn.append(self.knn[i])\n",
    "            chosen_weight.append(self.weight[i])\n",
    "        self.knn = chosen_knn\n",
    "        self.weight = chosen_weight\n",
    "        \n",
    "        return action_chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PNA Main Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PNA Main function** is responsible for initiating the PNA agent, environment and handling agent-environment interaction. It consists of a non-terminate inner loop that direct agent decision while also giving reward and next state from the environment. This inner loop will only break after the agent successfully get out of the environment, which in this case the mountain or if it is taking too long to converge. The outer loop can also be created to control the number of episodes which the agent will perform before the main function ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main Function script to test PNA agent\n",
    "\"\"\"\n",
    "\n",
    "# Initialise the environment and the agent\n",
    "size = 1000 # size of the q_storage \n",
    "agent = PNAAgent(size)\n",
    "mountain_car_environment = MountainCarEnvironment(agent)\n",
    "\n",
    "# Iterate the process, train the agent (training_iteration episodes)\n",
    "training_iteration = 1000\n",
    "for i in range(training_iteration):\n",
    "    step = 0\n",
    "    mountain_car_environment.reset()\n",
    "    while (True):\n",
    "        action = agent.selectAction()\n",
    "        step += 1\n",
    "        next_state = mountain_car_environment.nextState(action)\n",
    "    \n",
    "        # Change agent current state and getting reward\n",
    "        agent.state = next_state\n",
    "        immediate_reward = mountain_car_environment.calculateReward()\n",
    "    \n",
    "        # Test for successful learning\n",
    "        if (immediate_reward == 0):\n",
    "            print(\"Successfully get out of the mountain {} times. Steps taken for this episode: {}\".format(i + 1, step))\n",
    "            agent.TDUpdate(immediate_reward)\n",
    "            break\n",
    "        \n",
    "        # Update using Q Learning and kNN\n",
    "        agent.TDUpdate(immediate_reward)\n",
    "        \n",
    "        # Prevent too long convergence        \n",
    "        if (step > 100000):\n",
    "            print(\"Too many steps in this episode.\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
