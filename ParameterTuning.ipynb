{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Tuning for KNN-TD(0)\n",
    "\n",
    "*Author: Maleakhi Agung Wijaya*  \n",
    "*Date Created: 5/01/2018*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main purpose of this program is to tune the best k parameter/ number of nearest neighbours to use for the  KNN-TD approach described and implemented on [PNA.ipynb](https://github.com/maleakhiw/Pessimistic-Neighbourhood-Aggregation-for-States-in-Reinforcement-Learning/blob/master/PNA.ipynb). The main approach that is used for this purpose is by trying k (number of nearest neighbours) starting from 4 up to considering all points as neighbours. From there, results are generated and inspected for each k which we will choose the best k leading to convergence. The environment that will be used is Mountain Car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mountain Car Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MountainCarEnvironment:\n",
    "    \"\"\"\n",
    "    Description: Environment for Mountain Car problem, adapted from Sutton and Barto's Introduction to Reinforcement Learning.\n",
    "    Author: Maleakhi Agung Wijaya\n",
    "    \"\"\"\n",
    "    VELOCITY_BOUNDARIES = (-0.07, 0.07)\n",
    "    POSITION_BOUNDARIES = (-1.2, 0.6) \n",
    "    \n",
    "    INITIAL_VELOCITY = 0.0\n",
    "    INITIAL_POSITION = -0.5\n",
    "    \n",
    "    REWARD_STEP = -1\n",
    "    REWARD_TERMINAL = 0\n",
    "   \n",
    "    # Constructor for MountainCarEnvironment\n",
    "    # Input: agent for the MountainCarEnvironment\n",
    "    # Output: MountainCarEnvironment object\n",
    "    def __init__(self, car):\n",
    "        self.car = car\n",
    "        self.reset()\n",
    "        \n",
    "    # Compute next state (feature)\n",
    "    # Output: [new velocity, new position]\n",
    "    def nextState(self, action):\n",
    "        # Get current state (velocity, position) and the action chosen by the agent\n",
    "        velocity = self.car.state[0]\n",
    "        position = self.car.state[1]\n",
    "        \n",
    "        # Calculate the new velocity and new position\n",
    "        velocity += action * 0.001 + math.cos(3*position) * (-0.0025)\n",
    "        # Consider boundary for velocity\n",
    "        if (velocity < MountainCarEnvironment.VELOCITY_BOUNDARIES[0]):\n",
    "            velocity = MountainCarEnvironment.VELOCITY_BOUNDARIES[0]\n",
    "        elif (velocity > MountainCarEnvironment.VELOCITY_BOUNDARIES[1]):\n",
    "            velocity = MountainCarEnvironment.VELOCITY_BOUNDARIES[1]\n",
    "            \n",
    "        position += velocity\n",
    "        # Consider boundary for position\n",
    "        if (position < MountainCarEnvironment.POSITION_BOUNDARIES[0]):\n",
    "            position = MountainCarEnvironment.POSITION_BOUNDARIES[0]\n",
    "            velocity = 0\n",
    "        elif (position > MountainCarEnvironment.POSITION_BOUNDARIES[1]):\n",
    "            position = MountainCarEnvironment.POSITION_BOUNDARIES[1]\n",
    "        \n",
    "        new_state = [velocity, position]\n",
    "        return(new_state)\n",
    "    \n",
    "    # Reset to the initial state   \n",
    "    def reset(self):\n",
    "        self.car.state[0] = MountainCarEnvironment.INITIAL_VELOCITY\n",
    "        self.car.state[1] = MountainCarEnvironment.INITIAL_POSITION\n",
    "        \n",
    "    # Give reward for each of the chosen action, depending on what the next state that the agent end up in\n",
    "    # Output: terminal state = 0, non-terminal state = -1\n",
    "    def calculateReward(self):\n",
    "        # Get current position of the agent\n",
    "        position = self.car.state[1]\n",
    "        \n",
    "        # Determine the reward given\n",
    "        if (position >= MountainCarEnvironment.POSITION_BOUNDARIES[1]):\n",
    "            return(MountainCarEnvironment.REWARD_TERMINAL)\n",
    "        else:\n",
    "            return(MountainCarEnvironment.REWARD_STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KNNAgent:\n",
    "    \"\"\"\n",
    "    Description: Mountain Car problem agent based on kNN-TD(0) algorithm \n",
    "    Author: Maleakhi Agung Wijaya\n",
    "    \"\"\"\n",
    "    INITIAL_VELOCITY = 0.0\n",
    "    INITIAL_POSITION = -0.5\n",
    "    INITIAL_VALUE = -1\n",
    "    \n",
    "    ACTIONS = [-1, 0, 1]\n",
    "    GAMMA = 0.995\n",
    "    EPSILON = 0.05\n",
    "    \n",
    "    INDEX_DISTANCE = 0\n",
    "    INDEX_ORIGINAL = 1\n",
    "    INDEX_WEIGHT = 2\n",
    "    \n",
    "    REWARD_STEP = -1\n",
    "    REWARD_TERMINAL = 0\n",
    "    \n",
    "    # Constructor\n",
    "    # Input: size of the storage for previous Q values, parameters for how many neighbours which the agent will choose\n",
    "    def __init__(self, size, k):\n",
    "        self.state = [KNNAgent.INITIAL_VELOCITY, KNNAgent.INITIAL_POSITION]\n",
    "        self.q_storage = []\n",
    "        self.k = k # fixed number of nearest neighbours that we will used\n",
    "        self.alpha = 1 # will be decaying and change later\n",
    "        \n",
    "        # Storage of the k nearest neighbour (data) and weight (inverse of distance) for a particular step\n",
    "        self.knn = []\n",
    "        self.weight = []\n",
    "        \n",
    "        # Initialise the storage with random point \n",
    "        for i in range(size):\n",
    "            initial_action = random.randint(-1, 1)\n",
    "            initial_state = [random.uniform(-0.07, 0.07), random.uniform(-1.2, 0.6)]\n",
    "            \n",
    "            # Each data on the array will consist of state, action pair + value\n",
    "            data = {\"state\": initial_state, \"value\": KNNAgent.INITIAL_VALUE, \"action\": initial_action}\n",
    "            self.q_storage.append(data)\n",
    "    \n",
    "    # Find all index for a given value\n",
    "    # Input: value, list to search\n",
    "    # Output: list of all index where you find that value on the list\n",
    "    def findAllIndex(self, value, list_value):\n",
    "        indices = []\n",
    "        for i in range(len(list_value)):\n",
    "              if (value == list_value[i]):\n",
    "                    indices.append(i)\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    # Standardise feature vector given\n",
    "    # Input: feature vector to be standardised\n",
    "    # Output: standardised feature vector\n",
    "    def standardiseState(self, state):\n",
    "        standardised_state = []\n",
    "        \n",
    "        # The number is taken from VELOCITY_BOUNDARIES and POSITION_BOUNDARIES using normal standardisation formula\n",
    "        standardised_velocity = 2 * ((state[0]+0.07) / (0.07+0.07)) - 1\n",
    "        standardised_position = 2 * ((state[1]+1.2) / (0.6+1.2)) - 1\n",
    "        \n",
    "        standardised_state.append(standardised_velocity)\n",
    "        standardised_state.append(standardised_position)\n",
    "        \n",
    "        return(standardised_state)\n",
    "    \n",
    "    # Calculate Euclidean distance between 2 vectors\n",
    "    # Input: 2 feature vectors\n",
    "    # Output: distance between them\n",
    "    def calculateDistance(self, vector1, vector2):\n",
    "        return(math.sqrt((vector1[0]-vector2[0])**2 + (vector1[1]-vector2[1])**2))\n",
    "    \n",
    "    # Calculate total weight\n",
    "    # Input: list of weights\n",
    "    # Output: total weight\n",
    "    def calculateTotalWeight(self, weight_list):\n",
    "        total_weight = 0\n",
    "        for i in range(len(weight_list)):\n",
    "            total_weight += weight_list[i][KNNAgent.INDEX_WEIGHT]\n",
    "        \n",
    "        return(total_weight)\n",
    "    \n",
    "    # Apply the kNN algorithm for feature vector and store the data point on the neighbours array\n",
    "    # Input: feature vector of current state, actions array consisting of all possible actions, list that will store knn data and weights data\n",
    "    # Output: vector containing the value of taking each action (left, neutral, right)\n",
    "    def kNNTD(self, state, actions, knn_list, weight_list):\n",
    "        approximate_action = []\n",
    "        \n",
    "        # Get the standardised version of state\n",
    "        standardised_state = self.standardiseState(state)\n",
    "        \n",
    "        # Loop through every element in the storage array and only calculate for particular action\n",
    "        for action in actions:\n",
    "            temp = [] # array consisting of tuple (distance, original index, weight) for each point in the q_storage\n",
    "            for i in range(len(self.q_storage)):\n",
    "                data = self.q_storage[i]\n",
    "                # Only want to calculate the nearest neighbour state which has the same action\n",
    "                if (data[\"action\"] == action):\n",
    "                    vector_2 = data[\"state\"]\n",
    "                    standardised_vector_2 = self.standardiseState(vector_2)\n",
    "                    distance = self.calculateDistance(standardised_state, standardised_vector_2)\n",
    "                    index = i\n",
    "                    weight = 1 / (1+distance**2) # weight formula\n",
    "            \n",
    "                    # Create the tuple and append that to temp\n",
    "                    temp.append(tuple((distance, index, weight)))\n",
    "                else:\n",
    "                    continue\n",
    "        \n",
    "            # After we finish looping through all of the point and calculating the standardise distance,\n",
    "            # Sort the tuple based on the distance and only take k of it and append that to the neighbours array\n",
    "            # We also need to calculate the total weight to make it into valid probability that we can compute it's expectation\n",
    "            sorted_temp = sorted(temp, key=lambda x: x[0])\n",
    "            for i in range(self.k):\n",
    "                try:\n",
    "                    weight_list.append(sorted_temp[i])\n",
    "                    knn_list.append(self.q_storage[sorted_temp[i][KNNAgent.INDEX_ORIGINAL]])\n",
    "                except IndexError:\n",
    "                    sys.exit(0)\n",
    "            \n",
    "            # Calculate the expected value of the action and append it to the approximate_action array\n",
    "            expected_value = 0\n",
    "            total_weight = self.calculateTotalWeight(weight_list[(action+1)*self.k:(action+1)*self.k + self.k])\n",
    "            for i in range((action+1)*self.k, (action+1)*self.k + self.k):\n",
    "                weight = weight_list[i][KNNAgent.INDEX_WEIGHT]\n",
    "                probability = weight / total_weight\n",
    "                expected_value += probability * knn_list[i][\"value\"]\n",
    "                \n",
    "            approximate_action.append(expected_value)\n",
    "        \n",
    "        return(approximate_action)\n",
    "    \n",
    "    # Select which action to choose, whether left, neutral, or right (using epsilon greedy)\n",
    "    # Output: -1 (left), 0 (neutral), 1 (right)\n",
    "    def selectAction(self):\n",
    "        # First call the knn-td algorithm to determine the value of each Q(s,a) pairs\n",
    "        action_value = self.kNNTD(self.state, KNNAgent.ACTIONS, self.knn, self.weight)\n",
    "        \n",
    "        # Use the epsilon-greedy method to choose value\n",
    "        random_number = random.uniform(0.0, 1.0)\n",
    "        if (random_number <= KNNAgent.EPSILON):\n",
    "            action_chosen = random.randint(-1, 1)\n",
    "        else:\n",
    "            # Return the action with highest Q(s,a)\n",
    "            possible_index = self.findAllIndex(max(action_value), action_value)\n",
    "            action_chosen = possible_index[random.randrange(len(possible_index))] - 1\n",
    "        \n",
    "        # Only store chosen data in the knn and weight list\n",
    "        # Clearance step\n",
    "        chosen_knn = []\n",
    "        chosen_weight = []\n",
    "        for i in range(self.k*(action_chosen+1), self.k*(action_chosen+1) + self.k):\n",
    "            chosen_knn.append(self.knn[i])\n",
    "            chosen_weight.append(self.weight[i])\n",
    "        self.knn = chosen_knn\n",
    "        self.weight = chosen_weight\n",
    "\n",
    "        return action_chosen\n",
    "    \n",
    "    # Calculate TD target based on Q Learning/ SARSAMAX\n",
    "    # Input: Immediate reward based on what the environment gave\n",
    "    # Output: TD target based on off policy Q learning\n",
    "    def calculateTDTarget(self, immediate_reward):\n",
    "        # Consider condition on the final state, return 0 immediately\n",
    "        if (immediate_reward == KNNAgent.REWARD_TERMINAL):\n",
    "            return(immediate_reward)\n",
    "        \n",
    "        knn_prime = []\n",
    "        weight_prime = []\n",
    "        action_value = self.kNNTD(self.state, KNNAgent.ACTIONS, knn_prime, weight_prime)\n",
    "        \n",
    "        return(immediate_reward + KNNAgent.GAMMA*max(action_value))\n",
    "    \n",
    "    # Q learning TD updates on every neighbours on the kNN based on the contribution that are calculated using probability weight\n",
    "    # Input: Immediate reward based on what the environment gave\n",
    "    def TDUpdate(self, immediate_reward, alpha):\n",
    "        self.alpha = alpha\n",
    "        # First, calculate the TD target\n",
    "        td_target = self.calculateTDTarget(immediate_reward)\n",
    "        \n",
    "        # Iterate every kNN and update using Q learning method based on the weighting\n",
    "        total_weight = self.calculateTotalWeight(self.weight)\n",
    "        for i in range(len(self.weight)):\n",
    "            index = self.weight[i][KNNAgent.INDEX_ORIGINAL]\n",
    "            probability = self.weight[i][KNNAgent.INDEX_WEIGHT] / total_weight\n",
    "            \n",
    "            # Begin updating\n",
    "            td_error = td_target - self.q_storage[index][\"value\"]\n",
    "            self.q_storage[index][\"value\"] = self.q_storage[index][\"value\"] + self.alpha*td_error*probability\n",
    "        \n",
    "        self.cleanList() # clean list to prepare for another step\n",
    "            \n",
    "    # Clear the knn list and also the weight list\n",
    "    def cleanList(self):\n",
    "        self.knn = []\n",
    "        self.weight = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate decaying alphas\n",
    "# Input: minimum alpha, number of episodes\n",
    "# Output: list containing alpha\n",
    "def generateAlphas(minimum_alpha, n_episodes):\n",
    "    return(np.linspace(1.0, MIN_ALPHA, N_EPISODES))\n",
    "\n",
    "N_EPISODES = 50\n",
    "MIN_ALPHA = 0.02\n",
    "alphas = generateAlphas(MIN_ALPHA, N_EPISODES)\n",
    "\n",
    "# Initialise the environment and the agent\n",
    "size = 1000 # size of the q_storage \n",
    "k = 4 # knn parameter, this is just for initialisation, but later will be change below\n",
    "agent = KNNAgent(size, k)\n",
    "mountain_car_environment = MountainCarEnvironment(agent)\n",
    "\n",
    "# Check the largest k that can be chosen based on distribution of action point\n",
    "check_k = [0, 0, 0]\n",
    "for data in agent.q_storage:\n",
    "    check_k[data[\"action\"]+1] += 1\n",
    "\n",
    "largest_k_chosen = min(check_k)\n",
    "smallest_k_chosen = 4\n",
    "\n",
    "# Store initial q storage\n",
    "initial_q_storage = agent.q_storage\n",
    "\n",
    "# Store number of steps for each k\n",
    "k_step = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k_param in range(smallest_k_chosen, largest_k_chosen + 1):\n",
    "    # Iterate the process, train the agent (training_iteration episodes)\n",
    "    training_iteration = N_EPISODES\n",
    "    total_step = 0\n",
    "    agent.k = k_param\n",
    "    agent.q_storage = initial_q_storage\n",
    "    useless_k = False\n",
    "    for i in range(training_iteration):\n",
    "        step = 0\n",
    "        alpha = alphas[i]\n",
    "        mountain_car_environment.reset()\n",
    "        while (True):\n",
    "            action = agent.selectAction()\n",
    "            next_state = mountain_car_environment.nextState(action)\n",
    "        \n",
    "            # Change agent current state and getting reward\n",
    "            agent.state = next_state\n",
    "            immediate_reward = mountain_car_environment.calculateReward()\n",
    "            step += 1\n",
    "        \n",
    "            # Test for successful learning\n",
    "            if (immediate_reward == MountainCarEnvironment.REWARD_TERMINAL):\n",
    "                agent.TDUpdate(immediate_reward, alpha)\n",
    "                total_step += step\n",
    "                break\n",
    "        \n",
    "            # Update using Q Learning and kNN\n",
    "            agent.TDUpdate(immediate_reward, alpha)\n",
    "            \n",
    "            # Prevent not converge at all\n",
    "            if (step >= 500000):\n",
    "                useless_k = True\n",
    "                total_step = sys.maxsize\n",
    "                break\n",
    "        \n",
    "        if (useless_k):\n",
    "            break\n",
    "    \n",
    "    # After finishing all episodes required, calculate how many step taken during that period\n",
    "    k_step.append(total_step)\n",
    "    \n",
    "    # Graph dynamically\n",
    "    clear_output(wait=True)\n",
    "    y = k_step\n",
    "    x = np.arange(1, len(y) + 1)\n",
    "    plt.scatter(x, y)\n",
    "    plt.title(\"Total Steps vs K\", fontsize=16)\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Total Steps\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9223372036854775807\n"
     ]
    }
   ],
   "source": [
    "print(sys.maxsize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
